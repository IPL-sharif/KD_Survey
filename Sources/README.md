# KD_survey

## A Comprehensive Survey on Knowledge Distillation


## Table of Content
[SOURCES]()


### Logit-based Distillation
**Weak-to-strong 3d object detection with x-ray distillation**, CVPR 2024, [ :link: ][ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Gambashidze_Weak-to-Strong_3D_Object_Detection_with_X-Ray_Distillation_CVPR_2024_paper.html) [ :octocat: ](https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools)
- **Distilling the Knowledge in a Neural Network**, Arxiv 2015, [ :link: ](https://arxiv.org/abs/1503.02531)
- **Deep Model Compression: Distilling Knowledge from Noisy Teachers**. Arxiv 2016, [ :link: ](https://arxiv.org/abs/1610.09650)[ :octocat: ](https://github.com/chengshengchan/model_compression1)
- **Improved knowledge distillation via teacher assistant**, AAAI 2020, [ :link: ](https://arxiv.org/abs/1902.03393)[ :octocat: ](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
- **Reducing the Teacher-Student Gap via Spherical Knowledge Distillation**, Arxiv 2021, [ :link: ](https://arxiv.org/abs/2010.07485), Arxiv 2021. [ :octocat: ](https://github.com/forjiuzhou/Spherical-Knowledge-Distillation)
- **Decoupled knowledge distillation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2203.08679), CVPR 2022. [ :octocat: ](https://github.com/megvii-research/mdistiller)
- **NormKD: Normalized Logits for Knowledge Distillation**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2308.00520), Arxiv 2023. [ :octocat: ](https://github.com/gizi1/NormKD)
- **VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.15781), NeurIPS 2023. [ :octocat: ](https://github.com/Hao840/vanillaKD)
- **Boosting Knowledge Distillation via Intra-Class Logit Distribution Smoothing**, T-CSVT, [ :link: ](https://ieeexplore.ieee.org/document/10292885), T-CSVT 2023.
- **Exploring the knowledge transferred by response-based teacher-student distillation**, ACM MM 2023, [ :link: ](https://dl.acm.org/doi/10.1145/3581783.3612162), ACM MM 2023.
- **Student-friendly knowledge distillation**, Knowledge-Based Systems 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0950705124005495), Knowledge-Based Systems 2024.
- **NTCE-KD: Non-Target-Class-Enhanced Knowledge Distillation**, Sensors 2024, [ :link: ](https://www.mdpi.com/1424-8220/24/11/3617), Sensors 2024.
- **Generous teacher: Good at distilling knowledge for student learning**, Image and Vision Computing 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0262885624003044), Image and Vision Computing 2024. [ :octocat: ](https://github.com/EifelTing/Generous-Teacher)
- **Logit Standardization in Knowledge Distillation**, CVPR 2024, [ :link: ](https://arxiv.org/abs/2403.01427), CVPR 2024. [ :octocat: ](https://github.com/sunshangquan/logit-standardization-KD)
- **Scale Decoupled Distillation**, CVPR 2024, [ :link: ](https://arxiv.org/abs/2403.13512), CVPR 2024. [ :octocat: ](https://github.com/shicaiwei123/SDD-CVPR2024)
- **Understanding the Role of the Projector in Knowledge Distillation**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2303.11098), AAAI 2024. [ :octocat: ](https://github.com/roymiles/Simple-Recipe-Distillation)
- **BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://arxiv.org/abs/2306.08075), WACV 2024. [ :octocat: ](https://github.com/AkideLiu/BPKD)
- **Decoupled Kullback-Leibler Divergence Loss**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2305.13948), NeurIPS 2024.
- **Knowledge Distillation Based on Transformed Teacher Matching**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2402.11148), ICLR 2024.


### Feature-based Distillation


- **FitNets: Hints for Thin Deep Nets**, ICLR 2015, [ :link: ](https://arxiv.org/abs/1412.6550), ICLR 2015. [ :octocat: ](https://github.com/adri-romsor/FitNets)
- **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer**, ICLR 2017, [ :link: ](https://arxiv.org/abs/1612.03928), ICLR 2017. [ :octocat: ](https://github.com/szagoruyko/attention-transfer)
- **Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**, Arxiv 2017, [ :link: ](https://arxiv.org/abs/1707.01219), Arxiv 2017. [ :octocat: ](https://github.com/TuSimple/neuron-selectivity-transfer)
- **Paraphrasing Complex Network: Network Compression via Factor Transfer**, NeurIPS 2018, [ :link: ](https://arxiv.org/abs/1802.04977), NeurIPS 2018. [ :octocat: ](https://github.com/Jangho-Kim/Factor-Transfer-pytorch)
- **Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons**, AAAI2019, [ :link: ](https://arxiv.org/abs/1811.03233), AAAI 2019. [ :octocat: ](https://github.com/bhheo/AB_distillation)
- **Knowledge Adaptation for Efficient Semantic Segmentation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1903.04688), CVPR 2019. 
- **A Comprehensive Overhaul of Feature Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1904.01866), ICCV 2019. [ :octocat: ](https://github.com/clovaai/overhaul-distillation)
- **Channel Distillation: Channel-Wise Attention for Knowledge Distillation**, Arxiv 2020, [ :link: ](https://arxiv.org/abs/2006.01683), Arxiv 2020.
- **Distilling Knowledge via Knowledge Review**, CVPR 2021, [ :link: ](https://arxiv.org/abs/2104.09044), CVPR 2021. [ :octocat: ](https://github.com/dvlab-research/ReviewKD)
- **Cross-Layer Distillation with Semantic Calibration**, AAAI 2021, [ :link: ](https://arxiv.org/abs/2012.03236), AAAI 2021. [ :octocat: ](https://github.com/DefangChen/SemCKD)
- **ALP-KD: Attention-Based Layer Projection for Knowledge Distillation**, AAAI 2021, [ :link: ](https://arxiv.org/abs/2012.14022), AAAI 2021. 
- **Channel-Wise Knowledge Distillation for Dense Prediction**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Shu_Channel-Wise_Knowledge_Distillation_for_Dense_Prediction_ICCV_2021_paper.html), ICCV 2021. [ :octocat: ](https://github.com/drilistbox/CWD)
- **Masked Generative Distillation**, ECCV 2022, [ :link: ](https://arxiv.org/abs/2205.01529), ECCV 2022. [ :octocat: ](https://github.com/yzd-v/MGD)
- **A Simple and Generic Framework for Feature Distillation via Channel-wise Transformation**, ICCV 2023, [ :link: ](https://arxiv.org/abs/2303.13212), ICCV 2023. 
- **NORM: Knowledge Distillation via N-to-One Representation Matching**, ICLR 2023, [ :link: ](https://arxiv.org/abs/2305.13803), ICLR 2023. [ :octocat: ](https://github.com/OSVAI/NORM)
- **Knowledge Diffusion for Distillation**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.15712), NeurIPS 2024. [ :octocat: ](https://github.com/hunto/DiffKD)
- **FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://arxiv.org/abs/2208.14143), WACV 2024. [ :octocat: ](https://github.com/jianlong-yuan/FAKD)
- **Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation**, WACV 2024, [ :link: ](https://ieeexplore.ieee.org/document/10484265), WACV 2024. 
- **Attention-guided Feature Distillation for Semantic Segmentation**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2403.05451), Arxiv 2024. [ :octocat: ](https://github.com/AmirMansurian/AttnFD)


### Similarity-based Distillation

- **A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning**, CVPR 2017, [ :link: ](https://ieeexplore.ieee.org/document/8100237), CVPR 2017. 
- **Learning from Multiple Teacher Networks**, SIGGKD 2017, [ :link: ](https://dl.acm.org/doi/10.1145/3097983.3098135), SIGKDD 2017. 
- **Improving Fast Segmentation With Teacher-student Learning**, BMVC 2018, [ :link: ](https://arxiv.org/abs/1810.08476), BMVC 2018. 
- **Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification**, IJCAI 2018, [ :link: ](https://arxiv.org/abs/1804.10069), IJCAI 2018. [ :octocat: ](https://github.com/zcrwind/ss-graph-distillation)
- **Structured Knowledge Distillation for Semantic Segmentation**, CVPR 2019, [ :link: ](https://ieeexplore.ieee.org/document/8954081), CVPR 2019. [ :octocat: ](https://github.com/irfanICMLL/structure_knowledge_distillation)
- **Knowledge Distillation via Instance Relationship Graph**, CVPR 2019, [ :link: ](https://ieeexplore.ieee.org/document/8953802), CVPR 2019.
- **Correlation Congruence for Knowledge Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1904.01802), ICCV 2019. 
- **Relational Knowledge Distillation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1904.05068), CVPR 2019. [ :octocat: ](https://github.com/lenscloth/RKD)
- **Similarity-Preserving Knowledge Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1907.09682), ICCV 2019.
- **Intra-class Feature Variation Distillation for Semantic Segmentation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21), ECCV 2020. [ :octocat: ](https://github.com/YukangWang/IFVD)
- **Improving Knowledge Distillation via Category Structure**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58604-1_13), ECCV 2020. [ :octocat: ](https://github.com/xeanzheng/CSKD)
- **Learning student networks via feature embedding**, T-NNLS 2020, [ :link: ](https://ieeexplore.ieee.org/document/9007474), T-NNLS 2020.
- **Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2202.03680), ICCV 2021. [ :octocat: ](https://github.com/ADLab-AutoDrive/ICKD)
- **Double Similarity Distillation for Semantic Image Segmentation**, TIP 2021, [ :link: ](https://ieeexplore.ieee.org/document/9444191), TIP 2021.
- **Cross-Image Relational Knowledge Distillation for Semantic Segmentation**, CVPR 2020, [ :link: ](https://arxiv.org/abs/2204.06986), CVPR 2022. [ :octocat: ](https://github.com/winycg/CIRKD)
- **Knowledge Distillation from A Stronger Teacher**, NeurIPS 2022, [ :link: ](https://arxiv.org/abs/2205.10536), NeurIPS 2022. [ :octocat: ](https://github.com/hunto/DIST_KD)
- **Distilling Inter-Class Distance for Semantic Segmentation**, IJCAI 2022, [ :link: ](https://arxiv.org/abs/2205.03650), IJCAI 2022.
- **Channel Correlation Distillation for Compact Semantic Segmentation**, IJPRAI 2023, [ :link: ](https://www.worldscientific.com/doi/abs/10.1142/S0218001423500040?srsltid=AfmBOooHGx4UY1SPvq9awB5BcrJVvEOT0HVvNWafWJx8B4erkOQfFNr3), IJPRAI 2023.
- **Local structure consistency and pixel-correlation distillation for compact semantic segmentation**, Applied Intelligence 2023, [ :link: ](https://link.springer.com/article/10.1007/s10489-022-03656-4), Applied Intelligence 2023. 
- **PRRD: Pixel-Region Relation Distillation For Efficient Semantic Segmentation**, ICASSP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10094967/), ICASSP 2023.
- **BCKD: Block-Correlation Knowledge Distillation**, ICIP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10222195/), ICIP 2023. 
- **A New Similarity-Based Relational Knowledge Distillation Method**, ICASSP 2024, [ :link: ](https://ieeexplore.ieee.org/document/10447596/), ICASSP 2024. 
- **Similarity Knowledge Distillation with Calibrated Mask**, ICASSP 2024, [ :link: ](https://cmsworkshops.com/ICASSP2024/view_paper.php?PaperNum=3324), ICASSP 2024. 
- **Global Instance Relation Distillation for convolutional neural network compression**, Neural Computing 2024, [ :link: ](https://link.springer.com/article/10.1007/s00521-024-09635-9), Neural Computing 2024.
- **Cross-View Consistency Regularisation for Knowledge Distillation**, ACM MM 2024, [ :link: ](https://dl.acm.org/doi/10.1145/3664647.3681206), ACM MM 2024.
- **Knowledge Distillation via Inter- and Intra-Samples Relation Transferring**, FSKD 2024, [ :link: ](https://ieeexplore.ieee.org/document/10702218), FSKD 2024.

