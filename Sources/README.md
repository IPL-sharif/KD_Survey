# Sources

*  [Logit-based Distillation](#Logit-based-Distillation)
*  [Feature-based Distillation](#Feature-based-Distillation)
*  [Similarity-based Distillation](#Similarity-based-Distillation)
---

### Logit-based Distillation
- **Distilling the Knowledge in a Neural Network**, Arxiv 2015, [ :link: ](https://arxiv.org/abs/1503.02531)[ :octocat: ](https://github.com/shriramsb/Distilling-the-Knowledge-in-a-Neural-Network)
- **Deep Model Compression: Distilling Knowledge from Noisy Teachers**. Arxiv 2016, [ :link: ](https://arxiv.org/abs/1610.09650)[ :octocat: ](https://github.com/chengshengchan/model_compression1)
- **Improved knowledge distillation via teacher assistant**, AAAI 2020, [ :link: ](https://arxiv.org/abs/1902.03393)[ :octocat: ](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
- **Reducing the Teacher-Student Gap via Spherical Knowledge Distillation**, Arxiv 2021, [ :link: ](https://arxiv.org/abs/2010.07485)[ :octocat: ](https://github.com/forjiuzhou/Spherical-Knowledge-Distillation)
- **Decoupled knowledge distillation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2203.08679)[ :octocat: ](https://github.com/megvii-research/mdistiller)
- **NormKD: Normalized Logits for Knowledge Distillation**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2308.00520)[ :octocat: ](https://github.com/gizi1/NormKD)
- **VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.15781)[ :octocat: ](https://github.com/Hao840/vanillaKD)
- **Boosting Knowledge Distillation via Intra-Class Logit Distribution Smoothing**, T-CSVT 2023, [ :link: ](https://ieeexplore.ieee.org/document/10292885)
- **Exploring the knowledge transferred by response-based teacher-student distillation**, ACM MM 2023, [ :link: ](https://dl.acm.org/doi/10.1145/3581783.3612162)
- **Student-friendly knowledge distillation**, Knowledge-Based Systems 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0950705124005495)
- **NTCE-KD: Non-Target-Class-Enhanced Knowledge Distillation**, Sensors 2024, [ :link: ](https://www.mdpi.com/1424-8220/24/11/3617)
- **Generous teacher: Good at distilling knowledge for student learning**, Image and Vision Computing 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0262885624003044) [ :octocat: ](https://github.com/EifelTing/Generous-Teacher)
- **Logit Standardization in Knowledge Distillation**, CVPR 2024, [ :link: ](https://arxiv.org/abs/2403.01427)[ :octocat: ](https://github.com/sunshangquan/logit-standardization-KD)
- **Scale Decoupled Distillation**, CVPR 2024, [ :link: ](https://arxiv.org/abs/2403.13512)[ :octocat: ](https://github.com/shicaiwei123/SDD-CVPR2024)
- **Understanding the Role of the Projector in Knowledge Distillation**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2303.11098)[ :octocat: ](https://github.com/roymiles/Simple-Recipe-Distillation)
- **BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://arxiv.org/abs/2306.08075) [ :octocat: ](https://github.com/AkideLiu/BPKD)
- **Decoupled Kullback-Leibler Divergence Loss**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2305.13948)
- **Knowledge Distillation Based on Transformed Teacher Matching**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2402.11148)


### Feature-based Distillation


- **FitNets: Hints for Thin Deep Nets**, ICLR 2015, [ :link: ](https://arxiv.org/abs/1412.6550)[ :octocat: ](https://github.com/adri-romsor/FitNets)
- **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer**, ICLR 2017, [ :link: ](https://arxiv.org/abs/1612.03928)[ :octocat: ](https://github.com/szagoruyko/attention-transfer)
- **Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**, Arxiv 2017, [ :link: ](https://arxiv.org/abs/1707.01219)[ :octocat: ](https://github.com/TuSimple/neuron-selectivity-transfer)
- **Paraphrasing Complex Network: Network Compression via Factor Transfer**, NeurIPS 2018, [ :link: ](https://arxiv.org/abs/1802.04977)[ :octocat: ](https://github.com/Jangho-Kim/Factor-Transfer-pytorch)
- **Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons**, AAAI2019, [ :link: ](https://arxiv.org/abs/1811.03233)[ :octocat: ](https://github.com/bhheo/AB_distillation)
- **Knowledge Adaptation for Efficient Semantic Segmentation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1903.04688)
- **A Comprehensive Overhaul of Feature Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1904.01866)[ :octocat: ](https://github.com/clovaai/overhaul-distillation)
- **Channel Distillation: Channel-Wise Attention for Knowledge Distillation**, Arxiv 2020, [ :link: ](https://arxiv.org/abs/2006.01683)
- **Distilling Knowledge via Knowledge Review**, CVPR 2021, [ :link: ](https://arxiv.org/abs/2104.09044)[ :octocat: ](https://github.com/dvlab-research/ReviewKD)
- **Cross-Layer Distillation with Semantic Calibration**, AAAI 2021, [ :link: ](https://arxiv.org/abs/2012.03236)[ :octocat: ](https://github.com/DefangChen/SemCKD)
- **ALP-KD: Attention-Based Layer Projection for Knowledge Distillation**, AAAI 2021, [ :link: ](https://arxiv.org/abs/2012.14022) 
- **Channel-Wise Knowledge Distillation for Dense Prediction**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Shu_Channel-Wise_Knowledge_Distillation_for_Dense_Prediction_ICCV_2021_paper.html)[ :octocat: ](https://github.com/drilistbox/CWD)
- **Masked Generative Distillation**, ECCV 2022, [ :link: ](https://arxiv.org/abs/2205.01529)[ :octocat: ](https://github.com/yzd-v/MGD)
- **A Simple and Generic Framework for Feature Distillation via Channel-wise Transformation**, ICCV 2023, [ :link: ](https://arxiv.org/abs/2303.13212)
- **NORM: Knowledge Distillation via N-to-One Representation Matching**, ICLR 2023, [ :link: ](https://arxiv.org/abs/2305.13803)[ :octocat: ](https://github.com/OSVAI/NORM)
- **Knowledge Diffusion for Distillation**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.15712)[ :octocat: ](https://github.com/hunto/DiffKD)
- **FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://arxiv.org/abs/2208.14143)[ :octocat: ](https://github.com/jianlong-yuan/FAKD)
- **Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation**, WACV 2024, [ :link: ](https://ieeexplore.ieee.org/document/10484265)
- **Attention-guided Feature Distillation for Semantic Segmentation**, Arxiv 2024, [ :link: ](https://arxiv.org/abs/2403.05451)[ :octocat: ](https://github.com/AmirMansurian/AttnFD)


### Similarity-based Distillation

- **A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning**, CVPR 2017, [ :link: ](https://ieeexplore.ieee.org/document/8100237)
- **Learning from Multiple Teacher Networks**, SIGGKD 2017, [ :link: ](https://dl.acm.org/doi/10.1145/3097983.3098135)
- **Improving Fast Segmentation With Teacher-student Learning**, BMVC 2018, [ :link: ](https://arxiv.org/abs/1810.08476)
- **Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification**, IJCAI 2018, [ :link: ](https://arxiv.org/abs/1804.10069)[ :octocat: ](https://github.com/zcrwind/ss-graph-distillation)
- **Structured Knowledge Distillation for Semantic Segmentation**, CVPR 2019, [ :link: ](https://ieeexplore.ieee.org/document/8954081)[ :octocat: ](https://github.com/irfanICMLL/structure_knowledge_distillation)
- **Knowledge Distillation via Instance Relationship Graph**, CVPR 2019, [ :link: ](https://ieeexplore.ieee.org/document/8953802)
- **Correlation Congruence for Knowledge Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1904.01802)
- **Relational Knowledge Distillation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1904.05068)[ :octocat: ](https://github.com/lenscloth/RKD)
- **Similarity-Preserving Knowledge Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1907.09682)
- **Intra-class Feature Variation Distillation for Semantic Segmentation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21)[ :octocat: ](https://github.com/YukangWang/IFVD)
- **Improving Knowledge Distillation via Category Structure**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58604-1_13)[ :octocat: ](https://github.com/xeanzheng/CSKD)
- **Learning student networks via feature embedding**, T-NNLS 2020, [ :link: ](https://ieeexplore.ieee.org/document/9007474)
- **Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2202.03680)[ :octocat: ](https://github.com/ADLab-AutoDrive/ICKD)
- **Double Similarity Distillation for Semantic Image Segmentation**, TIP 2021, [ :link: ](https://ieeexplore.ieee.org/document/9444191)
- **Cross-Image Relational Knowledge Distillation for Semantic Segmentation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2204.06986)[ :octocat: ](https://github.com/winycg/CIRKD)
- **Knowledge Distillation from A Stronger Teacher**, NeurIPS 2022, [ :link: ](https://arxiv.org/abs/2205.10536)[ :octocat: ](https://github.com/hunto/DIST_KD)
- **Distilling Inter-Class Distance for Semantic Segmentation**, IJCAI 2022, [ :link: ](https://arxiv.org/abs/2205.03650)
- **Channel Correlation Distillation for Compact Semantic Segmentation**, IJPRAI 2023, [ :link: ](https://www.worldscientific.com/doi/abs/10.1142/S0218001423500040?srsltid=AfmBOooHGx4UY1SPvq9awB5BcrJVvEOT0HVvNWafWJx8B4erkOQfFNr3)
- **Local structure consistency and pixel-correlation distillation for compact semantic segmentation**, Applied Intelligence 2023, [ :link: ](https://link.springer.com/article/10.1007/s10489-022-03656-4)
- **PRRD: Pixel-Region Relation Distillation For Efficient Semantic Segmentation**, ICASSP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10094967/)
- **BCKD: Block-Correlation Knowledge Distillation**, ICIP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10222195/)
- **A New Similarity-Based Relational Knowledge Distillation Method**, ICASSP 2024, [ :link: ](https://ieeexplore.ieee.org/document/10447596/)
- **Similarity Knowledge Distillation with Calibrated Mask**, ICASSP 2024, [ :link: ](https://cmsworkshops.com/ICASSP2024/view_paper.php?PaperNum=3324)
- **Global Instance Relation Distillation for convolutional neural network compression**, Neural Computing 2024, [ :link: ](https://link.springer.com/article/10.1007/s00521-024-09635-9)
- **Cross-View Consistency Regularisation for Knowledge Distillation**, ACM MM 2024, [ :link: ](https://dl.acm.org/doi/10.1145/3664647.3681206)
- **Knowledge Distillation via Inter- and Intra-Samples Relation Transferring**, FSKD 2024, [ :link: ](https://ieeexplore.ieee.org/document/10702218)

