# KD_survey

## A Comprehensive Survey on Knowledge Distillation


## Table of Content
[SOURCES]()


### Logit-based Distillation

- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), Arxiv 2015.
- [Deep Model Compression: Distilling Knowledge from Noisy Teachers](https://arxiv.org/abs/1610.09650), Arxiv 2016. [[Code]](https://github.com/chengshengchan/model_compression1)
- [Improved knowledge distillation via teacher assistant](https://arxiv.org/abs/1902.03393), AAAI 2020. [[Code]](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
- [Reducing the Teacher-Student Gap via Spherical Knowledge Distillation](https://arxiv.org/abs/2010.07485), Arxiv 2021. [[Code]](https://github.com/forjiuzhou/Spherical-Knowledge-Distillation)
- [Decoupled knowledge distillation](https://arxiv.org/abs/2203.08679), CVPR 2022. [[Code]](https://github.com/megvii-research/mdistiller)
- [NormKD: Normalized Logits for Knowledge Distillation](https://arxiv.org/abs/2308.00520), Arxiv 2023. [[Code]](https://github.com/gizi1/NormKD)
- [VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale](https://arxiv.org/abs/2305.15781), NeurIPS 2023. [[Code]](https://github.com/Hao840/vanillaKD)
- [Boosting Knowledge Distillation via Intra-Class Logit Distribution Smoothing](https://ieeexplore.ieee.org/document/10292885), T-CSVT 2023.
- [Exploring the knowledge transferred by response-based teacher-student distillation](https://dl.acm.org/doi/10.1145/3581783.3612162), ACM MM 2023.
- [Student-friendly knowledge distillation](https://www.sciencedirect.com/science/article/abs/pii/S0950705124005495), Knowledge-Based Systems 2024.
- [NTCE-KD: Non-Target-Class-Enhanced Knowledge Distillation ](https://www.mdpi.com/1424-8220/24/11/3617), Sensors 2024.
- [Generous teacher: Good at distilling knowledge for student learning](https://www.sciencedirect.com/science/article/abs/pii/S0262885624003044), Image and Vision Computing 2024. [[Code]](https://github.com/EifelTing/Generous-Teacher)
- [Logit Standardization in Knowledge Distillation](https://arxiv.org/abs/2403.01427), CVPR 2024. [[Code]](https://github.com/sunshangquan/logit-standardization-KD)
- [Scale Decoupled Distillation](https://arxiv.org/abs/2403.13512), CVPR 2024. [[Code]](https://github.com/shicaiwei123/SDD-CVPR2024)
- [Understanding the Role of the Projector in Knowledge Distillation](https://arxiv.org/abs/2303.11098), AAAI 2024. [[Code]](https://github.com/roymiles/Simple-Recipe-Distillation)
- [BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation](https://arxiv.org/abs/2306.08075), WACV 2024. [[Code]](https://github.com/AkideLiu/BPKD)
- [Decoupled Kullback-Leibler Divergence Loss](https://arxiv.org/abs/2305.13948), NeurIPS 2024.
- [Knowledge Distillation Based on Transformed Teacher Matching](https://arxiv.org/abs/2402.11148), ICLR 2024.


### Feature-based Distillation


- [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550), ICLR 2015. [[Code]](https://github.com/adri-romsor/FitNets)
- [Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer](https://arxiv.org/abs/1612.03928), ICLR 2017. [[Code]](https://github.com/szagoruyko/attention-transfer)
- [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer](https://arxiv.org/abs/1707.01219), Arxiv 2017. [[Code]](https://github.com/TuSimple/neuron-selectivity-transfer)
- [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/abs/1802.04977), NeurIPS 2018. [[Code]](https://github.com/Jangho-Kim/Factor-Transfer-pytorch)
- [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons](https://arxiv.org/abs/1811.03233), AAAI 2019. [[Code]](https://github.com/bhheo/AB_distillation)
- [Knowledge Adaptation for Efficient Semantic Segmentation](https://arxiv.org/abs/1903.04688), CVPR 2019. 
- [A Comprehensive Overhaul of Feature Distillation](https://arxiv.org/abs/1904.01866), ICCV 2019. [[Code]](https://github.com/clovaai/overhaul-distillation)
- [Channel Distillation: Channel-Wise Attention for Knowledge Distillation](https://arxiv.org/abs/2006.01683), Arxiv 2020.
- [Distilling Knowledge via Knowledge Review](https://arxiv.org/abs/2104.09044), CVPR 2021. [[Code]](https://github.com/dvlab-research/ReviewKD)
- [Cross-Layer Distillation with Semantic Calibration](https://arxiv.org/abs/2012.03236), AAAI 2021. [[Code]](https://github.com/DefangChen/SemCKD)
- [ALP-KD: Attention-Based Layer Projection for Knowledge Distillation](https://arxiv.org/abs/2012.14022), AAAI 2021. 
- [Channel-Wise Knowledge Distillation for Dense Prediction](https://openaccess.thecvf.com/content/ICCV2021/html/Shu_Channel-Wise_Knowledge_Distillation_for_Dense_Prediction_ICCV_2021_paper.html), ICCV 2021. [[Code]](https://github.com/drilistbox/CWD)
- [Masked Generative Distillation](https://arxiv.org/abs/2205.01529), ECCV 2022. [[Code]](https://github.com/yzd-v/MGD)
- [A Simple and Generic Framework for Feature Distillation via Channel-wise Transformation](https://arxiv.org/abs/2303.13212), ICCV 2023. 
- [NORM: Knowledge Distillation via N-to-One Representation Matching](https://arxiv.org/abs/2305.13803), ICLR 2023. [[Code]](https://github.com/OSVAI/NORM)
- [FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation](https://arxiv.org/abs/2208.14143), WACV 2024. [[Code]](https://github.com/jianlong-yuan/FAKD)
- [Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation](https://ieeexplore.ieee.org/document/10484265), WACV 2024. 
- [Knowledge Diffusion for Distillation](https://arxiv.org/abs/2305.15712), NeurIPS 2024. [[Code]](https://github.com/hunto/DiffKD)
- [Attention-guided Feature Distillation for Semantic Segmentation](https://arxiv.org/abs/2403.05451), Arxiv 2024. [[Code]](https://github.com/AmirMansurian/AttnFD)


### Similarity-based Distillation

- [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning](https://ieeexplore.ieee.org/document/8100237), CVPR 2017. 
- [Learning from Multiple Teacher Networks](https://dl.acm.org/doi/10.1145/3097983.3098135), SIGKDD 2017. 
- [Improving Fast Segmentation With Teacher-student Learning](https://arxiv.org/abs/1810.08476), BMVC 2018. 
- [Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification](https://arxiv.org/abs/1804.10069), IJCAI 2018. [[Code]](https://github.com/zcrwind/ss-graph-distillation)
- [Structured Knowledge Distillation for Semantic Segmentation](https://ieeexplore.ieee.org/document/8954081), CVPR 2019. [[Code]](https://github.com/irfanICMLL/structure_knowledge_distillation)
- [Knowledge Distillation via Instance Relationship Graph](https://ieeexplore.ieee.org/document/8953802), CVPR 2019.
- [Correlation Congruence for Knowledge Distillation](https://arxiv.org/abs/1904.01802), ICCV 2019. 
- [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068), CVPR 2019. [[Code]](https://github.com/lenscloth/RKD)
- [Similarity-Preserving Knowledge Distillation](https://arxiv.org/abs/1907.09682), ICCV 2019.
- [Intra-class Feature Variation Distillation for Semantic Segmentation](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21), ECCV 2020. [[Code]](https://github.com/YukangWang/IFVD)
- [Improving Knowledge Distillation via Category Structure](https://link.springer.com/chapter/10.1007/978-3-030-58604-1_13), ECCV 2020. [[Code]](https://github.com/xeanzheng/CSKD)
- [Learning student networks via feature embedding](https://ieeexplore.ieee.org/document/9007474), T-NNLS 2020.
- [Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation](https://arxiv.org/abs/2202.03680), ICCV 2021. [[Code]](https://github.com/ADLab-AutoDrive/ICKD)
- [Double Similarity Distillation for Semantic Image Segmentation](https://ieeexplore.ieee.org/document/9444191), TIP 2021.
- [Cross-Image Relational Knowledge Distillation for Semantic Segmentation](https://arxiv.org/abs/2204.06986), CVPR 2022. [[Code]](https://github.com/winycg/CIRKD)
- [Knowledge Distillation from A Stronger Teacher](https://arxiv.org/abs/2205.10536), NeurIPS 2022. [[Code]](https://github.com/hunto/DIST_KD)
- [Distilling Inter-Class Distance for Semantic Segmentation](https://arxiv.org/abs/2205.03650), IJCAI 2022.
- [Channel Correlation Distillation for Compact Semantic Segmentation](https://www.worldscientific.com/doi/abs/10.1142/S0218001423500040?srsltid=AfmBOooHGx4UY1SPvq9awB5BcrJVvEOT0HVvNWafWJx8B4erkOQfFNr3), IJPRAI 2023.
- [Local structure consistency and pixel-correlation distillation for compact semantic segmentation](https://link.springer.com/article/10.1007/s10489-022-03656-4), Applied Intelligence 2023. 
- [PRRD: Pixel-Region Relation Distillation For Efficient Semantic Segmentation](https://ieeexplore.ieee.org/document/10094967/), ICASSP 2023.
- [BCKD: Block-Correlation Knowledge Distillation](https://ieeexplore.ieee.org/document/10222195/), ICIP 2023. 
- [A New Similarity-Based Relational Knowledge Distillation Method](https://ieeexplore.ieee.org/document/10447596/), ICASSP 2024. 
- [Similarity Knowledge Distillation with Calibrated Mask](https://cmsworkshops.com/ICASSP2024/view_paper.php?PaperNum=3324), ICASSP 2024. 
- [Global Instance Relation Distillation for convolutional neural network compression](https://link.springer.com/article/10.1007/s00521-024-09635-9), Neural Computing 2024.
- [Cross-View Consistency Regularisation for Knowledge Distillation](https://dl.acm.org/doi/10.1145/3664647.3681206), ACM MM 2024.
- [Knowledge Distillation via Inter- and Intra-Samples Relation Transferring](https://ieeexplore.ieee.org/document/10702218), FSKD 2024.

