# Schemes

*  [Offline Distillation](#Offline-Distillation)
*  [Adversarial Distillation](#Online-Distillation)
*  [Multi-teacher Distillation](#Self-Distillation)
---
## Offline Distillation

## Online Distillation

* Knowledge Distillation by On-the-Fly Native Ensemble - 2018 - [paper](https://proceedings.neurips.cc/paper/2018/hash/94ef7214c4a90790186e255304f8fd1f-Abstract.html) - [code](https://github.com/Lan1991Xu/ONE_NeurIPS2018)
* Online knowledge distillation via collaborative learning - 2020 - [paper](http://openaccess.thecvf.com/content_CVPR_2020/html/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.html) - [code](https://github.com/shaoeric/Online-Knowledge-Distillation-via-Collaborative-Learning)
* Improved Knowledge Distillation via Teacher Assistant - 2020 - [paper](https://aaai.org/ojs/index.php/AAAI/article/view/5963/5819) - [code](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
* Peer collaborative learning for online knowledge distillation - 2021 - [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17234)
* Online knowledge distillation for efficient pose estimation - 2021 - [paper](http://openaccess.thecvf.com/content/ICCV2021/html/Li_Online_Knowledge_Distillation_for_Efficient_Pose_Estimation_ICCV_2021_paper.html) - [code](https://github.com/zhengli97/OKDHP)
* Knowledge Distillation from A Stronger Teacher - 2022 - [paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/da669dfd3c36c93905a17ddba01eef06-Abstract-Conference.html) - [code](https://github.com/hunto/dist_kd)
* Parameter-Efficient and Student-Friendly Knowledge Distillation - 2022 - [paper](https://ieeexplore.ieee.org/abstract/document/10272648/)
* Online knowledge distillation via mutual contrastive learning for visual recognition - 2023 - [paper](https://ieeexplore.ieee.org/abstract/document/10073628/) - [code](https://github.com/winycg/mcl)
* Online Knowledge Distillation for Multi-task Learning - 2023 - [paper](https://openaccess.thecvf.com/content/WACV2023/html/Jacob_Online_Knowledge_Distillation_for_Multi-Task_Learning_WACV_2023_paper.html)
* Generalization Matters: Loss Minima Flattening via Parameter Hybridization
for Efficient Online Knowledge Distillation - 2023 - [paper](http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Generalization_Matters_Loss_Minima_Flattening_via_Parameter_Hybridization_for_Efficient_CVPR_2023_paper.html) - [code](https://github.com/tianlizhang/okdph)
* Online adversarial knowledge distillation for graph neural networks - 2024 - [paper](https://www.sciencedirect.com/science/article/pii/S0957417423021735) - [code](https://github.com/wangz3066/onlinedistillgcn)
* UDON: Universal Dynamic Online distillatioN for generic image representations - 2024 - [paper](https://arxiv.org/abs/2406.08332) - [code](https://github.com/nikosips/udon)


## Self-Distillation

