# Schemes

*  [Offline Distillation](#Offline-Distillation)
*  [Online Distillation](#Online-Distillation)
*  [Self-Distillation](#Self-Distillation)
---
## Offline Distillation

## Online Distillation

- **Knowledge Distillation by On-the-Fly Native Ensemble**, NeurIPS 2018, [ :link: ](https://proceedings.neurips.cc/paper/2018/hash/94ef7214c4a90790186e255304f8fd1f-Abstract.html)[ :octocat: ](https://github.com/Lan1991Xu/ONE_NeurIPS2018)
- **Online knowledge distillation via collaborative learning**, CVPR 2020, [ :link: ](http://openaccess.thecvf.com/content_CVPR_2020/html/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.html)[ :octocat: ](https://github.com/shaoeric/Online-Knowledge-Distillation-via-Collaborative-Learning)
- **Improved Knowledge Distillation via Teacher Assistant**, AAAI Conf. Artif. Intell. 2020, [ :link: ](https://aaai.org/ojs/index.php/AAAI/article/view/5963/5819)[ :octocat: ](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
- **Peer collaborative learning for online knowledge distillation**, AAAI Conf. Artif. Intell. 2021, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/17234)[ :octocat: ](https://github.com/shaoeric/Peer-Collaborative-Learning-for-Online-Knowledge-Distillation)
- **Online knowledge distillation for efficient pose estimation**, ICCV 2021, [ :link: ](http://openaccess.thecvf.com/content/ICCV2021/html/Li_Online_Knowledge_Distillation_for_Efficient_Pose_Estimation_ICCV_2021_paper.html)[ :octocat: ](https://github.com/zhengli97/OKDHP)
- **Knowledge Distillation from A Stronger Teacher**, NeurIPS 2022, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2022/hash/da669dfd3c36c93905a17ddba01eef06-Abstract-Conference.html)[ :octocat: ](https://github.com/hunto/dist_kd)
- **Parameter-Efficient and Student-Friendly Knowledge Distillation**, TMM 2022, [ :link: ](https://ieeexplore.ieee.org/abstract/document/10272648/)
- **Online knowledge distillation via mutual contrastive learning for visual recognition**, TPAMI 2023, [ :link: ](https://ieeexplore.ieee.org/abstract/document/10073628/)[ :octocat: ](https://github.com/winycg/mcl)
- **Online Knowledge Distillation for Multi-task Learning**, WACV 2023, [ :link: ](https://openaccess.thecvf.com/content/WACV2023/html/Jacob_Online_Knowledge_Distillation_for_Multi-Task_Learning_WACV_2023_paper.html)
- **Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation**, CVPR 2023, [ :link: ](http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Generalization_Matters_Loss_Minima_Flattening_via_Parameter_Hybridization_for_Efficient_CVPR_2023_paper.html)[ :octocat: ](https://github.com/tianlizhang/okdph)
- **Online adversarial knowledge distillation for graph neural networks**, Elsevier 2024, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0957417423021735)[ :octocat: ](https://github.com/wangz3066/onlinedistillgcn)
- **UDON: Universal Dynamic Online distillatioN for generic image representations**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2406.08332)[ :octocat: ](https://github.com/nikosips/udon)


## Self-Distillation
- **Born Again Neural Networks**, ICML 2018, [ :link: ](https://arxiv.org/abs/1805.04770)[ :octocat: ](https://github.com/nocotan/born_again_neuralnet)
- **Self-Referenced Deep Learning**, ACCV 2018, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-20890-5_19?fromPaywallRec=false)
- **Be your own teacher: Improve the performance of convolutional neural networks via self distillation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1905.08094)[ :octocat: ](https://github.com/luanyunteng/pytorch-be-your-own-teacher)
- **Data-distortion guided self-distillation for deep neural networks**, CVPR 2019, [ :link: ](https://dl.acm.org/doi/10.1609/aaai.v33i01.33015565)[ :octocat: ](https://github.com/youngerous/ddgsd-pytorch)
- **Self-Distillation as Instance-Specific Label Smoothing**, NeurIPS 2019, [ :link: ](https://arxiv.org/abs/2006.05065)[ :octocat: ](https://github.com/ZhiluZhang123/neurips\_2020\_distillation)
- **Self-Distillation Amplifies Regularization in Hilbert Space**, NeurIPS 2019, [ :link: ](https://arxiv.org/abs/2002.05715)
- **Self-distillation: Towards efficient and compact neural networks**, TPAMI 2021, [ :link: ](https://ieeexplore.ieee.org/document/9381661)[ :octocat: ](https://github.com/Frank-Jin54/self_distillation)
- **Self-Knowledge Distillation with Progressive Refinement of Targets**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2006.12000)[ :octocat: ](https://github.com/lgcnsai/PS-KD-Pytorch)
- **From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels**, ICCV 2023, [ :link: ](https://arxiv.org/abs/2303.13005)[ :octocat: ](https://github.com/yzd-v/cls_KD)
- **Deep Contrastive Representation Learning With Self-Distillation**, T-ETCI 2023, [ :link: ](https://ieeexplore.ieee.org/document/10233880)
- **Iterative Graph Self-Distillation**, T-CDE 2023, [ :link: ](https://arxiv.org/abs/2010.12609)
- **Neighbor self-knowledge distillation**, Information Science 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0020025523014445) 
- **Tolerant Self-Distillation for image classification**, Neural Networks 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0893608024001394)
- **Dual teachers for self-knowledge distillation**, Pattern Recognition 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0031320324001730)
- **Self-Distillation via Intra-Class Compactness**, PRCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-981-97-8487-5_10)
- **EPSD: Early Pruning with Self-Distillation for Efficient Model Compression**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2402.00084)
- **A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation**, TIP 2024, [ :link: ](https://arxiv.org/abs/2403.03483)[ :octocat: ](https://github.com/LirongWu/TGS)
- **Restructuring the Teacher and Student in Self-Distillation**, TIP 2024, [ :link: ](https://ieeexplore.ieee.org/document/10693311)
- **A Feature Map Fusion Self-Distillation Scheme for Image Classification Networks**, Electronics 2025, [ :link: ](https://www.mdpi.com/2079-9292/14/1/182) 

