## Distillation Applications
*  [Large Language Models](#Large-Language-Models)
*  [Self-Supervised Learning](#Self-Supervised-Learning)
---


### Large Language Models
- **TinyBERT: Distilling BERT for Natural Language Understanding**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1909.10351)[ :octocat: ](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)
- **Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1910.01108)
- **Patient Knowledge Distillation for BERT Model Compression**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1908.09355)[ :octocat: ](https://github.com/intersun/PKD-for-BERT-Model-Compression)
- **Well-read students learn better: On the importance of pre-training compact models**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1908.08962)[ :octocat: ](https://github.com/ricardordb/bert)
- **Xtremedistil: Multi-stage distillation for massive multilingual models**, Arxiv 2020, [ :link: ](https://aclanthology.org/2020.acl-main.202/)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**, ACL 2020, [ :link: ](https://aclanthology.org/2020.acl-main.195/)[ :octocat: ](https://github.com/lonePatient/MobileBert_PyTorch)
- **MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2002.10957)
- **xtremedistiltransformers: Task transfer for task-agnostic distillation**, Arxiv 2021, [ :link: ](https://arxiv.org/abs/2106.04563)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **Explanations from Large Language Models Make Small Reasoners Better**, Arxiv 2022, [ :link: ](https://arxiv.org/abs/2210.06726)
- **BERT Learns to Teach: Knowledge Distillation with Meta Learning**, ACL 2022, [ :link: ](https://arxiv.org/abs/2106.04570)[ :octocat: ](https://github.com/JetRunner/MetaDistil)
- **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2305.02301)[ :octocat: ](https://github.com/google-research/distilling-step-by-step)
- **Large language models are reasoning teachers**, ACL 2023, [ :link: ](https://arxiv.org/abs/2212.10071)[ :octocat: ](https://github.com/itsnamgyu/reasoning-teacher)
- **Less is More: Task-aware Layer-wise Distillation for Language Model Compression**, ICML 2023, [ :link: ](https://arxiv.org/abs/2210.01351)[ :octocat: ](https://github.com/cliang1453/task-aware-distillation)
- **MCC-KD: Multi-CoT Consistent Knowledge Distillation**, EMNLP 2023, [ :link: ](https://arxiv.org/abs/2310.14747)[ :octocat: ](https://github.com/homzer/MCC-KD)
- **Teaching Small Language Models to Reason**, ACL 2023, [ :link: ](https://aclanthology.org/2023.acl-short.151/)
- **Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step**, ACL 2023, [ :link: ](https://arxiv.org/abs/2306.14050)
- **Gkd: Generalized knowledge distillation for auto-regressive sequence models**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2306.13649)
- **SCOTT: Self-consistent chain-of-thought distillation**, NeurIPS 2023, [ :link: ](https://aclanthology.org/2023.acl-long.304/)[ :octocat: ](https://github.com/wangpf3/consistent-CoT-distillation)
- **Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents**, EMNLP 2023, [ :link: ](https://aclanthology.org/2023.emnlp-main.342/)[ :octocat: ](https://github.com/kyle8581/DialogueCoT)
- **MiniLLM: Knowledge Distillation of Large Language Models**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.08543)[ :octocat: ](https://github.com/microsoft/LMOps/tree/main/minillm)
- **On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.13649)
- **PaD: Program-aided Distillation Specializes Large Models in Reasoning**, ACL 2024, [ :link: ](https://arxiv.org/abs/2305.13888)
- **Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2312.12832)[ :octocat: ](https://github.com/Yiwei98/TDG)
- **PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation**, Arxiv 2024, [ :link: ](https://arxiv.org/abs/2406.09117)
- **Reverse Thinking Makes LLMs Stronger Reasoners**, Arxiv 2024, [ :link: ](https://arxiv.org/abs/2411.19865)




### Self-Supervised Learning

- **A simple framework for contrastive learning of visual representations**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/10.5555/3524938.3525087)[ :octocat: ](https://github.com/google-research/simclr)
- **Momentum contrast for unsupervised visual representation learning**, CVPR 2020, [ :link: ](https://arxiv.org/abs/1911.05722)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Improved Baselines with Momentum Contrastive Learning**, Arxiv 2020, [ :link: ](https://arxiv.org/abs/2003.04297)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Bootstrap your own latent-a new approach to self-supervised learning**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2006.07733)[ :octocat: ](https://github.com/lucidrains/byol-pytorch)
- **Unsupervised learning of visual features by contrasting cluster assignments**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/abs/10.5555/3495724.3496555)[ :octocat: ](https://github.com/facebookresearch/swav)
- **Self-Supervised Learning of Pretext-Invariant Representations**,  CVPR 2020, [ :link: ](https://arxiv.org/abs/1912.01991)[ :octocat: ](https://github.com/akwasigroch/Pretext-Invariant-Representations)
- **CompRess: Self-Supervised Learning by Compressing Representations**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2010.14713)[ :octocat: ](https://github.com/UMBCvision/CompRess)
- **SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation**, BMVC 2021, [ :link: ](https://arxiv.org/abs/2201.05131)[ :octocat: ](https://github.com/UCDvision/simreg)
- **ISD: Self-Supervised Learning by Iterative Similarity Distillation**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2012.09259)[ :octocat: ](https://github.com/UMBCvision/ISD)
- **SEED: Self-supervised Distillation For Visual Representation**, ICLR 2021, [ :link: ](https://arxiv.org/abs/2101.04731)[ :octocat: ](https://github.com/zhangyifei01/SEED_ICLR21)
- **Simple Distillation Baselines for Improving Small Self-supervised Models**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2106.11304)[ :octocat: ](https://github.com/JindongGu/SimDis)
- **Emerging Properties in Self-Supervised Vision Transformers**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2104.14294)[ :octocat: ](https://github.com/facebookresearch/dino)
- **Bag of Instances Aggregation Boosts Self-supervised Distillation**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2107.01691)
- **DisCo: Remedying Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning**, ECCV 2022, [ :link: ](https://arxiv.org/abs/2104.09124)[ :octocat: ](https://github.com/Yuting-Gao/DisCo-pytorch)
- **Auxiliary Learning for Self-Supervised Video Representation via Similarity-Based Knowledge Distillation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2112.04011)[ :octocat: ](https://github.com/Plrbear/auxSKD)
- **Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2212.04500)[ :octocat: ](https://github.com/ruiwang2021/mvd)
- **Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2304.06461)[ :octocat: ](https://github.com/skyoux/mokd)
- **DINOv2: Learning Robust Visual Features without Supervision**, TMLR 2024, [ :link: ](https://arxiv.org/abs/2304.07193)[ :octocat: ](https://github.com/facebookresearch/dinov2)

