## Distillation Applications
*  [Large Language Models](#Large-Language-Models)
*  [Self-Supervised Learning](#Self-Supervised-Learning)
* [Knowledge Distillation in Visual Recognition](#knowledge-distillation-in-visual-recognition)


---


### Large Language Models
- **TinyBERT: Distilling BERT for Natural Language Understanding**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1909.10351)[ :octocat: ](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)
- **Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1910.01108)
- **Patient Knowledge Distillation for BERT Model Compression**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1908.09355)[ :octocat: ](https://github.com/intersun/PKD-for-BERT-Model-Compression)
- **Well-read students learn better: On the importance of pre-training compact models**, Arxiv 2019, [ :link: ](https://arxiv.org/abs/1908.08962)[ :octocat: ](https://github.com/ricardordb/bert)
- **Xtremedistil: Multi-stage distillation for massive multilingual models**, Arxiv 2020, [ :link: ](https://aclanthology.org/2020.acl-main.202/)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**, ACL 2020, [ :link: ](https://aclanthology.org/2020.acl-main.195/)[ :octocat: ](https://github.com/lonePatient/MobileBert_PyTorch)
- **MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2002.10957)
- **xtremedistiltransformers: Task transfer for task-agnostic distillation**, Arxiv 2021, [ :link: ](https://arxiv.org/abs/2106.04563)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **Explanations from Large Language Models Make Small Reasoners Better**, Arxiv 2022, [ :link: ](https://arxiv.org/abs/2210.06726)
- **BERT Learns to Teach: Knowledge Distillation with Meta Learning**, ACL 2022, [ :link: ](https://arxiv.org/abs/2106.04570)[ :octocat: ](https://github.com/JetRunner/MetaDistil)
- **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2305.02301)[ :octocat: ](https://github.com/google-research/distilling-step-by-step)
- **Large language models are reasoning teachers**, ACL 2023, [ :link: ](https://arxiv.org/abs/2212.10071)[ :octocat: ](https://github.com/itsnamgyu/reasoning-teacher)
- **Less is More: Task-aware Layer-wise Distillation for Language Model Compression**, ICML 2023, [ :link: ](https://arxiv.org/abs/2210.01351)[ :octocat: ](https://github.com/cliang1453/task-aware-distillation)
- **MCC-KD: Multi-CoT Consistent Knowledge Distillation**, EMNLP 2023, [ :link: ](https://arxiv.org/abs/2310.14747)[ :octocat: ](https://github.com/homzer/MCC-KD)
- **Teaching Small Language Models to Reason**, ACL 2023, [ :link: ](https://aclanthology.org/2023.acl-short.151/)
- **Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step**, ACL 2023, [ :link: ](https://arxiv.org/abs/2306.14050)
- **Gkd: Generalized knowledge distillation for auto-regressive sequence models**, Arxiv 2023, [ :link: ](https://arxiv.org/abs/2306.13649)
- **SCOTT: Self-consistent chain-of-thought distillation**, NeurIPS 2023, [ :link: ](https://aclanthology.org/2023.acl-long.304/)[ :octocat: ](https://github.com/wangpf3/consistent-CoT-distillation)
- **Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents**, EMNLP 2023, [ :link: ](https://aclanthology.org/2023.emnlp-main.342/)[ :octocat: ](https://github.com/kyle8581/DialogueCoT)
- **MiniLLM: Knowledge Distillation of Large Language Models**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.08543)[ :octocat: ](https://github.com/microsoft/LMOps/tree/main/minillm)
- **On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.13649)
- **PaD: Program-aided Distillation Specializes Large Models in Reasoning**, ACL 2024, [ :link: ](https://arxiv.org/abs/2305.13888)
- **Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2312.12832)[ :octocat: ](https://github.com/Yiwei98/TDG)
- **PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation**, Arxiv 2024, [ :link: ](https://arxiv.org/abs/2406.09117)
- **Reverse Thinking Makes LLMs Stronger Reasoners**, Arxiv 2024, [ :link: ](https://arxiv.org/abs/2411.19865)




### Self-Supervised Learning

- **A simple framework for contrastive learning of visual representations**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/10.5555/3524938.3525087)[ :octocat: ](https://github.com/google-research/simclr)
- **Momentum contrast for unsupervised visual representation learning**, CVPR 2020, [ :link: ](https://arxiv.org/abs/1911.05722)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Improved Baselines with Momentum Contrastive Learning**, Arxiv 2020, [ :link: ](https://arxiv.org/abs/2003.04297)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Bootstrap your own latent-a new approach to self-supervised learning**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2006.07733)[ :octocat: ](https://github.com/lucidrains/byol-pytorch)
- **Unsupervised learning of visual features by contrasting cluster assignments**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/abs/10.5555/3495724.3496555)[ :octocat: ](https://github.com/facebookresearch/swav)
- **Self-Supervised Learning of Pretext-Invariant Representations**,  CVPR 2020, [ :link: ](https://arxiv.org/abs/1912.01991)[ :octocat: ](https://github.com/akwasigroch/Pretext-Invariant-Representations)
- **CompRess: Self-Supervised Learning by Compressing Representations**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2010.14713)[ :octocat: ](https://github.com/UMBCvision/CompRess)
- **SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation**, BMVC 2021, [ :link: ](https://arxiv.org/abs/2201.05131)[ :octocat: ](https://github.com/UCDvision/simreg)
- **ISD: Self-Supervised Learning by Iterative Similarity Distillation**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2012.09259)[ :octocat: ](https://github.com/UMBCvision/ISD)
- **SEED: Self-supervised Distillation For Visual Representation**, ICLR 2021, [ :link: ](https://arxiv.org/abs/2101.04731)[ :octocat: ](https://github.com/zhangyifei01/SEED_ICLR21)
- **Simple Distillation Baselines for Improving Small Self-supervised Models**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2106.11304)[ :octocat: ](https://github.com/JindongGu/SimDis)
- **Emerging Properties in Self-Supervised Vision Transformers**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2104.14294)[ :octocat: ](https://github.com/facebookresearch/dino)
- **Bag of Instances Aggregation Boosts Self-supervised Distillation**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2107.01691)
- **DisCo: Remedying Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning**, ECCV 2022, [ :link: ](https://arxiv.org/abs/2104.09124)[ :octocat: ](https://github.com/Yuting-Gao/DisCo-pytorch)
- **Auxiliary Learning for Self-Supervised Video Representation via Similarity-Based Knowledge Distillation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2112.04011)[ :octocat: ](https://github.com/Plrbear/auxSKD)
- **Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2212.04500)[ :octocat: ](https://github.com/ruiwang2021/mvd)
- **Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2304.06461)[ :octocat: ](https://github.com/skyoux/mokd)
- **DINOv2: Learning Robust Visual Features without Supervision**, TMLR 2024, [ :link: ](https://arxiv.org/abs/2304.07193)[ :octocat: ](https://github.com/facebookresearch/dinov2)

## Knowledge Distillation in Visual Recognition

### Object Detection

- **Learning Efficient Object Detection Models with Knowledge Distillation**, NIPS 2017,  [ :link: ](https://papers.nips.cc/paper_files/paper/2017/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html)
- **Distilling Object Detectors with Fine-grained Feature Imitation**, CVPR 2019, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Distilling_Object_Detectors_With_Fine-Grained_Feature_Imitation_CVPR_2019_paper.pdf)[ :octocat: ](https://github.com/twangnh/Distilling-Object-Detectors)
- **An end-to-end architecture for class-incremental object detection with knowledge distillation**, ICME 2019, [ :link: ](https://ieeexplore.ieee.org/document/8784755/)[ :octocat: ](https://github.com/jinyu121/CIOD)
- **LabelEnc: A New Intermediate Supervision Method for Object Detection**, ECCV 2020, [ :link: ](https://www.semanticscholar.org/paper/LabelEnc%3A-A-New-Intermediate-Supervision-Method-for-Hao-Liu/3e05ead93c516f1921ca337854ef30cf8474caa4)[ :octocat: ](https://github.com/megvii-model/LabelEnc)
- **MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection**, ECCV 2020, [ :link: ](https://dl.acm.org/doi/10.1007/978-3-030-58568-6_32)
- **Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors**, ICLR 2021, [ :link: ](https://papertalk.org/papertalks/28434)[ :octocat: ](https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021)
- **Distilling Object Detectors via Decoupled Features**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Distilling_Object_Detectors_via_Decoupled_Features_CVPR_2021_paper.pdf)[ :octocat: ](https://github.com/ggjy/DeFeat.pytorch)
- **General Instance Distillation for Object Detection**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/papers/Dai_General_Instance_Distillation_for_Object_Detection_CVPR_2021_paper.pdf)
- **Instance-Conditional Knowledge Distillation for Object Detection**, NIPS 2021, [ :link: ](https://papers.neurips.cc/paper/2021/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf)[ :octocat: ](https://github.com/MegEngine/ICD)
- **Improving Object Detection by Label Assignment Distillation**, WACV 2022, [ :link: ](https://openaccess.thecvf.com/content/WACV2022/papers/Nguyen_Improving_Object_Detection_by_Label_Assignment_Distillation_WACV_2022_paper.pdf)[ :octocat: ](https://github.com/cybercore-co-ltd/CoLAD)
- **Distilling image classifiers in object detectors**, NIPS 2021, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2021/file/082a8bbf2c357c09f26675f9cf5bcba3-Paper.pdf)[ :octocat: ](https://github.com/NVlabs/DICOD)
- **G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/papers/Yao_G-DetKD_Towards_General_Distillation_Framework_for_Object_Detectors_via_Contrastive_ICCV_2021_paper.pdf)
- **Distilling Object Detectors with Feature Richness**, NIPS 2021, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2021/file/29c0c0ee223856f336d7ea8052057753-Paper.pdf)[ :octocat: ](https://github.com/duzhixing/FRS)
- **Prediction-Guided Distillation for Dense Object Detection**, ECCV 2022, [ :link: ](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690123-supp.pdf)[ :octocat: ](https://github.com/ChenhongyiYang/PGD)
- **Focal and Global Knowledge Distillation for Detectors**, CVPR 2022, [ :link: ](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.pdf)[ :octocat: ](https://github.com/yzd-v/FGD)
- **GLAMD: Global and Local Attention Mask Distillation for Object Detectors**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20080-9_27)
- **Masked Generative Distillation**, ECCV 2022, [ :link: ]()[ :octocat: ]()
- **LGD: Label-guided Self-distillation for Object Detection**, AAAI 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_4)[ :octocat: ](https://github.com/yzd-v/MGD)
- **Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-Guided Feature Imitation**, AAAI 2022, [ :link: ](https://cdn.aaai.org/ojs/20018/20018-13-24031-1-2-20220628.pdf)
- **Task-balanced distillation for object detection**,  Pattern Recognition 2023, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0031320323000213)
- **Structured Knowledge Distillation for Accurate and Efficient Object Detection**, TPAMI 2023, [ :link: ](https://ieeexplore.ieee.org/document/10198386/)
- **Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes**, ICCV 2023, [ :link: ]()[ :octocat: ]()
- **Dual Relation Knowledge Distillation for Object Detection**, IJCAI 2023, [ :link: ]()
- **ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector**, CVPR 2023, [ :link: ]()
- **Bridging Cross-task Protocol Inconsistency for Distillation
in Dense Object Detection**, ICCV 2023, [ :link: ]()
- **Multi-level Logit Distillation**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- **Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection**, ICCV 2023, [ :link: ]()
- **Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection**, ICCV 2023, [ :link: ]()
- **UniKD: Universal Knowledge Distillation for Mimicking Homogeneous or Heterogeneous Object Detectors**, ICCV 2023, [ :link: ]()
- **Texture-Guided Saliency Distilling for Unsupervised Salient Object Detection**, CVPR 2023, [ :link: ]()
- **Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection**, CVPR 2023, [ :link: ]()
- **CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection**, CVPR 2024, [ :link: ]()[ :octocat: ]()
- **MSSD: multi-scale self-distillation for object detection**, Visual Intelligence  2024, [ :link: ]()
- **CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**, ICLR 2024, [ :link: ]()
- **Gradient-Guided Knowledge Distillation for Object Detectors**, WACV 2024, [ :link: ]()
- **Efficient Feature Distillation for Zero-Shot Annotation Object Detection**, WACV 2024, [ :link: ]()
- **Localization distillation for object detection**, IEEE TPAMI 2023, [ :link: ]()[ :octocat: ]()

### Super Resolution

- **Image Super-Resolution Using Knowledge Distillation**, ACCV 2018, [ :link: ]()
- **Feature-Affinity Based Knowledge Distillation for Efficient Image Super-Resolution**, ICIP 2020, [ :link: ]()[ :octocat: ]()
- **Learning with Privileged Information for Efficient Image Super-Resolution**, ECCV 2020, [ :link: ]()[ :octocat: ]()
- **Data-Free Knowledge Distillation for Image Super-Resolution**, CVPR 2021, [ :link: ]()
- **Towards Compact Single Image Super-Resolution via Contrastive Self-distillation**, IJCAI 2021, [ :link: ]()[ :octocat: ]()
- **DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution**, CVPRW 2023, [ :link: ]()[ :octocat: ]()
- **Generative Adversarial Super-Resolution at the edge with knowledge distillation**, Engineering Applications of Artificial Intelligence 2023, [ :link: ]()[ :octocat: ]()
- **Hybrid knowledge distillation from intermediate layers for efficient Single Image Super-Resolution**, Neurocomputing 2023, [ :link: ]()
- **Dual cross knowledge distillation for image super-resolution**, Journal of Visual Communication and Image Representation 2023, [ :link: ]()
- **Pairwise Distance Distillation for Unsupervised Real-World Image Super-Resolution**, ECCV 2024, [ :link: ]()[ :octocat: ]()
- **Knowledge Distillation for Single Image Super-Resolution via Contrastive Learning**, ICMR 2024, [ :link: ]()
- **MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution**, ECCV 2024, [ :link: ]()
- **You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation**, ECCV 2024, [ :link: ]()
- **Attention Guidance Distillation Network for Efficient Image Super-Resolution**, CVPRW 2024, [ :link: ]()[ :octocat: ]()
- **SinSR: Diffusion-Based Image Super-Resolution in a Single Step**, CVPR 2024, [ :link: ]()
- **OSFFNet: Omni-Stage Feature Fusion Network for Lightweight Image Super-Resolution**, AAAI 2024, [ :link: ]()
- **Semantic Super-Resolution via Self-Distillation and Adversarial Learning**,  IEEE Access 2024, [ :link: ]()

### Image Segmentation

- **Structured knowledge distillation for semantic segmentation**, CVPR 2019, [ :link: ]()[ :octocat: ]()
- **Knowledge Adaptation for Efficient Semantic Segmentation**, CVPR 2019, [ :link: ]()
- **Intra-class Feature Variation Distillation for Semantic Segmentation**, ECCV 2020, [ :link: ]()[ :octocat: ]()
- **Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation**, WACVW 2020, [ :link: ]()[ :octocat: ]()
- **Channel-wise Knowledge Distillation for Dense Prediction**, ICCV 2021, [ :link: ]()[ :octocat: ]()
- **Double Similarity Distillation for Semantic Image Segmentation**, IEEE Transactions on Image Processing (TIP) 2021, [ :link: ]()
- **Robust Semantic Segmentation With Multi-Teacher Knowledge Distillation**, IEEE Access 2021, [ :link: ]()
- **Cross-Image Relational Knowledge Distillation for Semantic Segmentation**, CVPR 2022, [ :link: ]()[ :octocat: ]()
- **Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation**, CVPR 2022, [ :link: ]()
- **Adaptive Perspective Distillation for Semantic Segmentation**, TPAMI 2022, [ :link: ]()[ :octocat: ]()
- **Masked Generative Distillation**, ECCV 2022, [ :link: ]()[ :octocat: ]()
- **Class Similarity Weighted Knowledge Distillation for Continual Semantic Segmentation**, CVPR 2022, [ :link: ]()
- **Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation**, CVPR 2022, [ :link: ]()
- **DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- **A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation**, ICCV 2023, [ :link: ]()
- **Endpoints Weight Fusion for Class Incremental Semantic Segmentation**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- **Multi-Task Learning with Knowledge Distillation for Dense Prediction**, ICCV 2023, [ :link: ]()
- **Hierarchical Dense Correlation Distillation for Few-Shot Segmentation**, CVPR 2023, [ :link: ]()
- **Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation**, CVPR 2023, [ :link: ]()
- **Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class**, CVPR 2023, [ :link: ]()
- **FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ]()
- **Knowledge Distillation for Efficient Instance Semantic Segmentation with Transformers**, CVPRW 2024, [ :link: ]()
- **Distilling efficient Vision Transformers from CNNs for semantic segmentation**, Pattern Recognition 2025, [ :link: ]()
- **CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**, ICLR 2024, [ :link: ]()[ :octocat: ]()
- **P2Seg: Pointly-supervised Segmentation via Mutual Distillation**, ICLR 2024, [ :link: ]()[ :octocat: ]()
- **Rethinking Knowledge Distillation With Raw Features for Semantic Segmentation**, WACV 2024, [ :link: ]()
- **BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ]()[ :octocat: ]()
- **Guided Distillation for Semi-Supervised Instance Segmentation**, WACV 2024, [ :link: ]()[ :octocat: ]()
- **AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation**, arxiv 2023, [ :link: ]()[ :octocat: ]()

### Depth Estimation

- **Knowledge Distillation for Fast and Accurate Monocular Depth Estimation on Mobile Devices**, CVPRW 2021, [ :link: ]()
- **Realtime single image depth perception in the wild with handheld devices**, Sensors 2021, [ :link: ]()
- **Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation**, ICCV 2021, [ :link: ]()
- **Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation**, ECCV 2022, [ :link: ]()
- **Attention-based depth distillation with 3d-aware positional encoding for monocular 3d object detection**, AAAI 2022, [ :link: ]()
- **Boosting Light-Weight Depth Estimation Via Knowledge Distillation**, KSEM 2023, [ :link: ]()[ :octocat: ]()
- **Urcdc-depth: Uncertainty rectified cross-distillation with cutflip for monocular depth estimation**, IEEE Transactions on Multimedia 2023, [ :link: ]()[ :octocat: ]()
- **Multi-task learning with knowledge distillation for dense prediction**, CVPR 2023, [ :link: ]()
- **Attention-Based Knowledge Distillation in Scene Recognition: The Impact of a DCT-Driven Loss**, IEEE Transactions on Circuits and Systems for Video Technology 2023, [ :link: ]()
- **Monocular Depth Estimation from a Fisheye Camera Based on Knowledge Distillation**, Sensors 2023, [ :link: ]()
- **3D Distillation: Improving Self-Supervised Monocular Depth Estimation on Reflective Surfaces**, ICCV 2023, [ :link: ]()
- **Multi-Task Learning with Knowledge Distillation for Dense Prediction**, ICCV 2023, [ :link: ]()
- **GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes**, ICCV 2023, [ :link: ]()
- **Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation**, NIPS 2024, [ :link: ]()[ :octocat: ]()
- **Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation**, ACCV 2024, [ :link: ]()
- ** MonoProb: Self-Supervised Monocular Depth Estimation With Interpretable Uncertainty **, WACV 2024, [ :link: ]()
- ** Monocular Depth Estimation via Self-Supervised Self-Distillation **, Sensors 2024, [ :link: ]()
- **MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation**, ICRA 2024, [ :link: ]()
- **Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency**, ICASSP 2024, [ :link: ]()

### Medical Image Analysis

- **HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical Image Classification**, Journal - Engineering Applications of Artificial Intelligence 2024, [ :link: ]()[ :octocat: ]()
- **ShapeKD: Shape Knowledge Distillation for Medical Image Segmentation**,   journal - Front. Comput. Sci 2024, [ :link: ]()[ :octocat: ]()
- **Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification**, MICCAI 2021, [ :link: ]()[ :octocat: ]()
- **Efficient Medical Image Segmentation Based on Knowledge Distillation**,  IEEE Transactions on Medical Imaging (TMI) 2021, [ :link: ]()[ :octocat: ]()
- **DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation**, MICCAI 2022, [ :link: ]()
- ** Efficient Biomedical Instance Segmentation via Knowledge Distillation**, MICCAI 2022, [ :link: ]()
- **Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification**, MICCAI 2022, [ :link: ]()
- **Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions**, MICCAI 2022, [ :link: ]()[ :octocat: ]()
- **Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation**, IEEE Trans. Med. Imaging 2022, [ :link: ]()
- **Deep Mutual Distillation for Semi-Supervised Medical Image Segmentation**, MICCAI 2023, [ :link: ]()[ :octocat: ]()
- **Distilling BlackBox to Interpretable models for Efficient Transfer Learning**, MICCAI 2023, [ :link: ]()[ :octocat: ]()
- **Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality**, MICCAI 2023, [ :link: ]()
- ** Self-distillation for surgical action recognition**, MICCAI 2023, [ :link: ]()
- **Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions**, MICCAI 2023, [ :link: ]()[ :octocat: ]()
- **Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models**, CVPR 2024, [ :link: ]()
- **Reverse Knowledge Distillation: Training a Large Model Using a Small One for Retinal Image Matching on Limited Data**, WACV 2024, [ :link: ]()
- **Learning Robust Shape Regularization for Generalizable Medical Image Segmentation**, IEEE Trans. Med. Imaging 2024, [ :link: ]()[ :octocat: ]()
- **Enhancing Medical Imaging with GANs Synthesizing Realistic Images from Limited Data**, ICETCI 2024, [ :link: ]()
- **Exploring Generalizable Distillation for Efficient Medical Image Segmentation**, IEEE Journal of Biomedical and Health Informatics 2024, [ :link: ]()[ :octocat: ]()
- **Confidence Matters: Enhancing Medical Image Classification Through Uncertainty-Driven Contrastive Self-Distillation**, MICCAI 2024, [ :link: ]()[ :octocat: ]()
- ** DES-SAM: Distillation-Enhanced Semantic SAM for Cervical Nuclear Segmentation with Box Annotation**, MICCAI 2024, [ :link: ]()[ :octocat: ]()
- **Hallucinated Style Distillation for Single Domain Generalization in Medical Image Segmentation**, MICCAI 2024, [ :link: ]()
- **Progressively Correcting Soft Labels via Teacher Team for Knowledge Distillation in Medical Image Segmentation**, MICCAI 2024, [ :link: ]()
- **Reprogramming Distillation for Medical Foundation Models**, MICCAI 2024, [ :link: ]()

### Object Tracking

- **There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge**, CVPR 2021, [ :link: ]()[ :octocat: ]()
- **Ensemble learning with siamese networks for visual tracking**, Neurocomputing 2021, [ :link: ]()
- **Distilled Siamese Networks for Visual Tracking**, TPAMI 2022, [ :link: ]()
- **SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking**, CVPR 2024, [ :link: ]()[ :octocat: ]()
- **Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline**, CVPR 2024, [ :link: ]()[ :octocat: ]()
- **Object Knowledge Distillation for Joint Detection and Tracking in Satellite Videos**, IEEE Transactions on Geoscience and Remote Sensing  2024, [ :link: ]()

### Face Recognition

- **Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection**, ICCV 2019, [ :link: ]()[ :octocat: ]()
- **MarginDistillation: distillation for margin-based softmax**, automation and remote control 2021, [ :link: ]()[ :octocat: ]()
- **Fair Feature Distillation for Visual Recognition**, CVPR 2021, [ :link: ]()
- **Rectifying the Data Bias in Knowledge Distillation**, ICCV 2021, [ :link: ]()
- **Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition**, ECCV 2022, [ :link: ]()
- **AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition**, ECCV 2024, [ :link: ]()[ :octocat: ]()
- **SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data**,  IJCB 2023, [ :link: ]()[ :octocat: ]()
- **Synthetic Gap Mitigation using Knowledge Distillation in Fair Face Recognition**, ECCV Workshop 2024, [ :link: ]()[ :octocat: ]()
- **MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition**, ECCV Workshop 2024, [ :link: ]()[ :octocat: ]()
- **Evaluation-oriented knowledge distillation for deep face recognition**, CVPR 2022, [ :link: ]()
- **Rethinking Feature-Based Knowledge Distillation for Face Recognition**, CVPR 2023, [ :link: ]()
- **Probabilistic Knowledge Distillation of Face Ensembles**, CVPR 2023, [ :link: ]()
- **Enhanced Face Recognition using Intra-class Incoherence Constraint**, ICLR 2024, [ :link: ]()
- **ProS: Facial Omni-Representation Learning via Prototype-Based Self-Distillation**, WACV 2024, [ :link: ]()
- **AI-KD: Towards Alignment Invariant Face Image Quality Assessment Using Knowledge Distillation**, IWBF 2024, [ :link: ]()[ :octocat: ]()

### Action recognition

- **Learning an augmented rgb representation with cross-modal knowledge distillation for action detection**, ICCV 2021, [ :link: ]()
- **Privileged Knowledge Distillation for Online Action Detection**, Pattern recognition 2022, [ :link: ]()
- **Multimodal Distillation for Egocentric Action Recognition**, ICCV 2023, [ :link: ]()[ :octocat: ]()
- **FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation**, ICCV 2023, [ :link: ]()
- **Decomposed Cross-Modal Distillation for RGB-Based Temporal Action Detection**, CVPR 2023, [ :link: ]()
- **Generative Model-Based Feature Knowledge Distillation for Action Recognition**, AAAI 2024, [ :link: ]()
- **FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition**, ICLR 2024, [ :link: ]()[ :octocat: ]()
- **Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition**, ICCV 2021, [ :link: ]()[ :octocat: ]()
- **Structural Knowledge Distillation for Efficient Skeleton-Based Action Recognition**, IEEE Trans. Image Processing 2021, [ :link: ]()[ :octocat: ]()

### Pose Estimation

- **DistilPose: Tokenized Pose Regression With Heatmap Distillation**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- **Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- **SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation**, CVPR 2024, [ :link: ]()[ :octocat: ]()
- **Online Knowledge Distillation for Efficient Pose Estimation**, ICCV 2021, [ :link: ]()[ :octocat: ]()
- **Effective Whole-Body Pose Estimation with Two-Stages Distillation**, ICCV 2023, [ :link: ]()[ :octocat: ]()
- **HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation**, Chinese Journal of Electronics 2023, [ :link: ]()
- **When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks **, CVPR 2021, [ :link: ]()[ :octocat: ]()
- **From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation**, CVPR 2021, [ :link: ]()[ :octocat: ]()
- **Combining Weight Pruning and Knowledge Distillation for CNN Compression**, CVPR Workshop 2021, [ :link: ]()
- **AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time**, IEEE TPAMI 2022, [ :link: ]()[ :octocat: ]()
- **ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation**, NeurIPS 2022, [ :link: ]()[ :octocat: ]()
- **Dynamic Kernel Distillation for Efficient Pose Estimation in Videos**, ICCV 2019, [ :link: ]()
- **Fast Human Pose Estimation**, CVPR 2019, [ :link: ]()[ :octocat: ]()

### Image retrieval

- **Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification**, AAAI 2020, [ :link: ]()
- **Robust re-identification by multiple views knowledge distillation**, ECCV 2020, [ :link: ]()
- **Relationship-Preserving Knowledge Distillation for Zero-Shot Sketch Based Image Retrieval**, ACMM 2021, [ :link: ]()
- **Asymmetric metric learning for knowledge transfer**, CVPR 2021, [ :link: ]()
- **Contextual Similarity Distillation for Asymmetric Image Retrieval**, CVPR 2022, [ :link: ]()
- **Large-to-small image resolution asymmetry in deep metric learning**, WACV 2023, [ :link: ]()
- **Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval**, CVPR 2023, [ :link: ]()[ :octocat: ]()
- ** D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval **, CVPR 2024, [ :link: ]()
- **LSTKC: Long Short-Term Knowledge Consolidation for Lifelong Person Re-identification**, AAAI 2024, [ :link: ]()
- **Pairwise difference relational distillation for object re-identification**, Pattern Recognition 2024, [ :link: ]()