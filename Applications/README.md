## Distillation Applications
*  [Large Language Models](#Large-Language-Models)
*  [Foundation Models](#foundation-models)
*  [Self-Supervised Learning](#Self-Supervised-Learning)
*  [Diffusion Models](#diffusion-models)
*  [Visual Recognition](#knowledge-distillation-in-visual-recognition)


---


## Large Language Models
- **TinyBERT: Distilling BERT for Natural Language Understanding**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1909.10351)[ :octocat: ](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)
- **Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1910.01108)
- **Patient Knowledge Distillation for BERT Model Compression**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1908.09355)[ :octocat: ](https://github.com/intersun/PKD-for-BERT-Model-Compression)
- **Well-read students learn better: On the importance of pre-training compact models**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1908.08962)[ :octocat: ](https://github.com/ricardordb/bert)
- **Xtremedistil: Multi-stage distillation for massive multilingual models**, arXiv 2020, [ :link: ](https://aclanthology.org/2020.acl-main.202/)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**, ACL 2020, [ :link: ](https://aclanthology.org/2020.acl-main.195/)[ :octocat: ](https://github.com/lonePatient/MobileBert_PyTorch)
- **MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2002.10957)
- **xtremedistiltransformers: Task transfer for task-agnostic distillation**, arXiv 2021, [ :link: ](https://arxiv.org/abs/2106.04563)[ :octocat: ](https://github.com/microsoft/xtreme-distil-transformers)
- **Explanations from Large Language Models Make Small Reasoners Better**, arXiv 2022, [ :link: ](https://arxiv.org/abs/2210.06726)
- **BERT Learns to Teach: Knowledge Distillation with Meta Learning**, ACL 2022, [ :link: ](https://arxiv.org/abs/2106.04570)[ :octocat: ](https://github.com/JetRunner/MetaDistil)
- **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**, arXiv 2023, [ :link: ](https://arxiv.org/abs/2305.02301)[ :octocat: ](https://github.com/google-research/distilling-step-by-step)
- **Large language models are reasoning teachers**, ACL 2023, [ :link: ](https://arxiv.org/abs/2212.10071)[ :octocat: ](https://github.com/itsnamgyu/reasoning-teacher)
- **Less is More: Task-aware Layer-wise Distillation for Language Model Compression**, ICML 2023, [ :link: ](https://arxiv.org/abs/2210.01351)[ :octocat: ](https://github.com/cliang1453/task-aware-distillation)
- **MCC-KD: Multi-CoT Consistent Knowledge Distillation**, EMNLP 2023, [ :link: ](https://arxiv.org/abs/2310.14747)[ :octocat: ](https://github.com/homzer/MCC-KD)
- **Teaching Small Language Models to Reason**, ACL 2023, [ :link: ](https://aclanthology.org/2023.acl-short.151/)
- **Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step**, ACL 2023, [ :link: ](https://arxiv.org/abs/2306.14050)
- **Gkd: Generalized knowledge distillation for auto-regressive sequence models**, arXiv 2023, [ :link: ](https://arxiv.org/abs/2306.13649)
- **SCOTT: Self-consistent chain-of-thought distillation**, NeurIPS 2023, [ :link: ](https://aclanthology.org/2023.acl-long.304/)[ :octocat: ](https://github.com/wangpf3/consistent-CoT-distillation)
- **Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents**, EMNLP 2023, [ :link: ](https://aclanthology.org/2023.emnlp-main.342/)[ :octocat: ](https://github.com/kyle8581/DialogueCoT)
- **MiniLLM: Knowledge Distillation of Large Language Models**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.08543)[ :octocat: ](https://github.com/microsoft/LMOps/tree/main/minillm)
- **On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2306.13649)
- **PaD: Program-aided Distillation Specializes Large Models in Reasoning**, ACL 2024, [ :link: ](https://arxiv.org/abs/2305.13888)
- **Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data**, AAAI 2024, [ :link: ](https://arxiv.org/abs/2312.12832)[ :octocat: ](https://github.com/Yiwei98/TDG)
- **PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2406.09117)
- **Reverse Thinking Makes LLMs Stronger Reasoners**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2411.19865)
- **UniCoTT: A Unified Framework for Structural Chain-of-Thought Distillation**, ICLR 2025, [ :link: ](https://openreview.net/forum?id=3baOKeI2EU)
- **MiniPLM: Knowledge Distillation for Pre-Training Language Models**, ICLR 2025, [ :link: ](https://arxiv.org/abs/2410.17215)[ :octocat: ](https://github.com/thu-coai/MiniPLM)


## Foundation Models
### Vision-Language Models (VLMs)
### Segment Anything (SAM)
### Multi-model Applications
### Vision Transformers


## Self-Supervised Learning

- **A simple framework for contrastive learning of visual representations**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/10.5555/3524938.3525087)[ :octocat: ](https://github.com/google-research/simclr)
- **Momentum contrast for unsupervised visual representation learning**, CVPR 2020, [ :link: ](https://arxiv.org/abs/1911.05722)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Improved Baselines with Momentum Contrastive Learning**, arXiv 2020, [ :link: ](https://arxiv.org/abs/2003.04297)[ :octocat: ](https://github.com/facebookresearch/moco)
- **Bootstrap your own latent-a new approach to self-supervised learning**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2006.07733)[ :octocat: ](https://github.com/lucidrains/byol-pytorch)
- **Unsupervised learning of visual features by contrasting cluster assignments**, NeurIPS 2020, [ :link: ](https://dl.acm.org/doi/abs/10.5555/3495724.3496555)[ :octocat: ](https://github.com/facebookresearch/swav)
- **Self-Supervised Learning of Pretext-Invariant Representations**,  CVPR 2020, [ :link: ](https://arxiv.org/abs/1912.01991)[ :octocat: ](https://github.com/akwasigroch/Pretext-Invariant-Representations)
- **CompRess: Self-Supervised Learning by Compressing Representations**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/2010.14713)[ :octocat: ](https://github.com/UMBCvision/CompRess)
- **SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation**, BMVC 2021, [ :link: ](https://arxiv.org/abs/2201.05131)[ :octocat: ](https://github.com/UCDvision/simreg)
- **ISD: Self-Supervised Learning by Iterative Similarity Distillation**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2012.09259)[ :octocat: ](https://github.com/UMBCvision/ISD)
- **SEED: Self-supervised Distillation For Visual Representation**, ICLR 2021, [ :link: ](https://arxiv.org/abs/2101.04731)[ :octocat: ](https://github.com/zhangyifei01/SEED_ICLR21)
- **Simple Distillation Baselines for Improving Small Self-supervised Models**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2106.11304)[ :octocat: ](https://github.com/JindongGu/SimDis)
- **Emerging Properties in Self-Supervised Vision Transformers**, ICCV 2021, [ :link: ](https://arxiv.org/abs/2104.14294)[ :octocat: ](https://github.com/facebookresearch/dino)
- **Bag of Instances Aggregation Boosts Self-supervised Distillation**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2107.01691)
- **DisCo: Remedying Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning**, ECCV 2022, [ :link: ](https://arxiv.org/abs/2104.09124)[ :octocat: ](https://github.com/Yuting-Gao/DisCo-pytorch)
- **Auxiliary Learning for Self-Supervised Video Representation via Similarity-Based Knowledge Distillation**, CVPR 2022, [ :link: ](https://arxiv.org/abs/2112.04011)[ :octocat: ](https://github.com/Plrbear/auxSKD)
- **Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2212.04500)[ :octocat: ](https://github.com/ruiwang2021/mvd)
- **Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2304.06461)[ :octocat: ](https://github.com/skyoux/mokd)
- **DINOv2: Learning Robust Visual Features without Supervision**, TMLR 2024, [ :link: ](https://arxiv.org/abs/2304.07193)[ :octocat: ](https://github.com/facebookresearch/dinov2)


## Diffusion Models
- **Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed**, arXiv 2021, [ :link: ](https://arxiv.org/abs/2101.02388)[ :octocat: ](https://github.com/tcl9876/Denoising_Student)
- **Progressive Distillation for Fast Sampling of Diffusion Models**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2202.00512)[ :octocat: ](https://github.com/Hramchenko/diffusion_distiller)
- **Accelerating diffusion sampling with classifier-based feature distillation**, ICME 2023, [ :link: ](https://arxiv.org/abs/2211.12039)[ :octocat: ]([https://github.com/facebookresearch/dinov2](https://github.com/zju-SWJ/RCFD))
- **Consistency Models**, PMLR 2023, [ :link: ](https://arxiv.org/abs/2303.01469)[ :octocat: ](https://github.com/openai/consistency_models)
- **Fast Sampling of Diffusion Models via Operator Learning**, PMLR 2023, [ :link: ](https://arxiv.org/abs/2211.13449)
- **Diffusion-GAN: Training GANs with Diffusion**, ICLR 2023, [ :link: ](https://arxiv.org/abs/2206.02262)[ :octocat: ](https://github.com/Zhendong-Wang/Diffusion-GAN)
- **ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.16213)[ :octocat: ](https://github.com/thu-ml/prolificdreamer)
- **Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models**, NeurIPS 2023, [ :link: ](https://arxiv.org/abs/2305.18455)[ :octocat: ](https://github.com/pkulwj1994/diff_instruct)
- **Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference**, arXiv 2023, [ :link: ](https://arxiv.org/abs/2310.04378)[ :octocat: ](https://github.com/luosiallen/latent-consistency-model)
- **Adversarial Diffusion Distillation**, ECCV 2024, [ :link: ](https://arxiv.org/abs/2311.17042)[ :octocat: ](https://github.com/leffff/adversarial-diffusion-distillation)
- **Improved Techniques for Training Consistency Models**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2310.14189)
- **Relational diffusion distillation for efficient image generation**, ACM MM 2024, [ :link: ](https://arxiv.org/abs/2410.07679)[ :octocat: ](https://github.com/cantbebetter2/RDD)
- **Multistep Distillation of Diffusion Models via Moment Matching**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2406.04103)
- **EM Distillation for One-step Diffusion Models**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2405.16852)
- **One-step Diffusion with Distribution Matching Distillation**, CVPR 2024, [ :link: ](https://arxiv.org/abs/2311.18828)[ :octocat: ](https://github.com/devrimcavusoglu/dmd)
- **Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2410.11081)[ :octocat: ](https://github.com/xandergos/sCM-mnist)
- **Consistency Models Made Easy**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2406.14548)[ :octocat: ](https://github.com/locuslab/ect)
- **TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2406.05768)[ :octocat: ](https://github.com/oppo-mente-lab/tlcm)



## Knowledge Distillation in Visual Recognition

### Object Detection

- **Learning Efficient Object Detection Models with Knowledge Distillation**, NIPS 2017,  [ :link: ](https://papers.nips.cc/paper_files/paper/2017/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html)
- **Distilling Object Detectors with Fine-grained Feature Imitation**, CVPR 2019, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2019/329300e928/1gys77GATfy)[ :octocat: ](https://github.com/twangnh/Distilling-Object-Detectors)
- **An end-to-end architecture for class-incremental object detection with knowledge distillation**, ICME 2019, [ :link: ](https://ieeexplore.ieee.org/document/8784755/)[ :octocat: ](https://github.com/jinyu121/CIOD)
- **LabelEnc: A New Intermediate Supervision Method for Object Detection**, ECCV 2020, [ :link: ](https://www.semanticscholar.org/paper/LabelEnc%3A-A-New-Intermediate-Supervision-Method-for-Hao-Liu/3e05ead93c516f1921ca337854ef30cf8474caa4)[ :octocat: ](https://github.com/megvii-model/LabelEnc)
- **MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection**, ECCV 2020, [ :link: ](https://dl.acm.org/doi/10.1007/978-3-030-58568-6_32)
- **Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors**, ICLR 2021, [ :link: ](https://papertalk.org/papertalks/28434)[ :octocat: ](https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021)
- **Distilling Object Detectors via Decoupled Features**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900c154/1yeJ0pkcoGQ)[ :octocat: ](https://github.com/ggjy/DeFeat.pytorch)
- **General Instance Distillation for Object Detection**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900h838/1yeJaBZflcc)
- **Instance-Conditional Knowledge Distillation for Object Detection**, NIPS 2021, [ :link: ](https://www.semanticscholar.org/paper/Instance-Conditional-Knowledge-Distillation-for-Kang-Zhang/0db2770e9942984c625a0d0cd7e6ac5a773a8f89)[ :octocat: ](https://github.com/MegEngine/ICD)
- **Improving Object Detection by Label Assignment Distillation**, WACV 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2022/091500b322/1B12HZyCZwY)[ :octocat: ](https://github.com/cybercore-co-ltd/CoLAD)
- **Distilling image classifiers in object detectors**, NIPS 2021, [ :link: ](https://dl.acm.org/doi/10.5555/3540261.3540341)[ :octocat: ](https://github.com/NVlabs/DICOD)
- **G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200d571/1BmIGpt2zT2)
- **Distilling Object Detectors with Feature Richness**, NIPS 2021, [ :link: ](https://dl.acm.org/doi/10.5555/3540261.3540660)[ :octocat: ](https://github.com/duzhixing/FRS)
- **Prediction-Guided Distillation for Dense Object Detection**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20077-9_8)[ :octocat: ](https://github.com/ChenhongyiYang/PGD)
- **Focal and Global Knowledge Distillation for Detectors**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600e633/1H0L69YPBXq)[ :octocat: ](https://github.com/yzd-v/FGD)
- **GLAMD: Global and Local Attention Mask Distillation for Object Detectors**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20080-9_27)
- **Masked Generative Distillation**, ECCV 2022, [ :link: ]()[ :octocat: ]()
- **LGD: Label-guided Self-distillation for Object Detection**, AAAI 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_4)[ :octocat: ](https://github.com/yzd-v/MGD)
- **Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-Guided Feature Imitation**, AAAI 2022, [ :link: ](https://cdn.aaai.org/ojs/20018/20018-13-24031-1-2-20220628.pdf)
- **Task-balanced distillation for object detection**,  Pattern Recognition 2023, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0031320323000213)
- **Structured Knowledge Distillation for Accurate and Efficient Object Detection**, TPAMI 2023, [ :link: ](https://ieeexplore.ieee.org/document/10198386/)
- **Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800g832/1TJf1Cum7jq)[ :octocat: ](https://github.com/ucas-vg/ssddet)
- **Dual Relation Knowledge Distillation for Object Detection**, IJCAI 2023, [ :link: ](https://dl.acm.org/doi/10.24963/ijcai.2023/142)
- **ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900t9723/1PORNw4p92E)
- **Multi-level Logit Distillation**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900y4276/1POVC9HfZks)[ :octocat: ](https://github.com/Jin-Ying/Multi-Level-Logit-Distillation)
- **Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection**, ICCV 2023, [ :link: ](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Bridging_Cross-task_Protocol_Inconsistency_for_Distillation_in_Dense_Object_Detection_ICCV_2023_paper.pdf)[ :octocat: ](https://github.com/TinyTigerPan/BCKD)
- **Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection**, ICCV 2023, [ :link: ](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Distilling_DETR_with_Visual-Linguistic_Knowledge_for_Open-Vocabulary_Object_Detection_ICCV_2023_paper.pdf)
- **UniKD: Universal Knowledge Distillation for Mimicking Homogeneous or Heterogeneous Object Detectors**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800g339/1TJeOT8Jnc4)
- **Texture-Guided Saliency Distilling for Unsupervised Salient Object Detection**, CVPR 2023, [ :link: ](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Texture-Guided_Saliency_Distilling_for_Unsupervised_Salient_Object_Detection_CVPR_2023_paper.pdf)[ :octocat: ](https://github.com/moothes/A2S-v2)
- **Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900l1186/1POPYYaayli)[ :octocat: ](https://github.com/LutingWang/OADP)
- **Localization distillation for object detection**, IEEE TPAMI 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600j397/1H0LhSBFO4U)[ :octocat: ](https://github.com/HikariTJU/LD)
- **CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CrossKD_Cross-Head_Knowledge_Distillation_for_Object_Detection_CVPR_2024_paper.pdf)[ :octocat: ](https://github.com/jbwang1997/CrossKD)
- **MSSD: multi-scale self-distillation for object detection**, Visual Intelligence  2024, [ :link: ](https://link.springer.com/article/10.1007/s44267-024-00040-3)
- **CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**, ICLR 2024, [ :link: ](https://wusize.github.io/publication/clipself)[ :octocat: ](https://github.com/wusize/CLIPSelf)
- **Gradient-Guided Knowledge Distillation for Object Detectors**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200a423/1W0f2CZ4xPO)
- **Efficient Feature Distillation for Zero-Shot Annotation Object Detection**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200a882/1W0cAEU3FVC)[ :octocat: ](https://github.com/dragonlzm/EZAD)

### Super Resolution

- **Image Super-Resolution Using Knowledge Distillation**, ACCV 2018, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-20890-5_34)
- **Feature-Affinity Based Knowledge Distillation for Efficient Image Super-Resolution**, ICIP 2020, [ :link: ](https://ieeexplore.ieee.org/document/9190917/)[ :octocat: ](https://github.com/Vincent-Hoo/Knowledge-Distillation-for-Super-resolution)
- **Learning with Privileged Information for Efficient Image Super-Resolution**, ECCV 2020, [ :link: ](https://cvlab.yonsei.ac.kr/projects/PISR/)[ :octocat: ](https://github.com/cvlab-yonsei/PISR)
- **Data-Free Knowledge Distillation for Image Super-Resolution**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900h848/1yeI7I8tpUQ)
- **Towards Compact Single Image Super-Resolution via Contrastive Self-distillation**, IJCAI 2021, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S003132032400253X)[ :octocat: ](https://github.com/Booooooooooo/CSD)
- **DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution**, CVPRW 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvprw/2023/024900b692/1PBxdD5eHLO)[ :octocat: ](https://github.com/xiumu00/DIPNet)
- **Generative Adversarial Super-Resolution at the edge with knowledge distillation**, Engineering Applications of Artificial Intelligence 2023, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0952197623005912)[ :octocat: ](https://github.com/PIC4SeR/EdgeSRGAN)
- **Hybrid knowledge distillation from intermediate layers for efficient Single Image Super-Resolution**, Neurocomputing 2023, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0925231223007154)
- **Dual cross knowledge distillation for image super-resolution**, Journal of Visual Communication and Image Representation 2023, [ :link: ](https://www.sciencedirect.com/science/article/pii/S1047320323001086)
- **Pairwise Distance Distillation for Unsupervised Real-World Image Super-Resolution**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-73397-0_25)[ :octocat: ](https://github.com/Yuehan717/PDD)
- **Knowledge Distillation for Single Image Super-Resolution via Contrastive Learning**, ICMR 2024, [ :link: ](https://dl.acm.org/doi/10.1145/3652583.3657606)
- **MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution**, ECCV 2024, [ :link: ](https://arxiv.org/abs/2404.09571)[ :octocat: ](https://github.com/YuxuanJJ/MTKD)
- **You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-73397-0_9)
- **Attention Guidance Distillation Network for Efficient Image Super-Resolution**, CVPRW 2024, [ :link: ](https://ieeexplore.ieee.org/document/10678094/)[ :octocat: ](https://github.com/daydreamer2024/AGDN)
- **SinSR: Diffusion-Based Image Super-Resolution in a Single Step**, CVPR 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000aa796/20hPWIvl8ti)[ :octocat: ](https://github.com/wyf0912/SinSR)
- **OSFFNet: Omni-Stage Feature Fusion Network for Lightweight Image Super-Resolution**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/28377)
- **Semantic Super-Resolution via Self-Distillation and Adversarial Learning**,  IEEE Access 2024, [ :link: ](https://ieeexplore.ieee.org/document/10379089/)

### Image Segmentation

- **Structured knowledge distillation for semantic segmentation**, CVPR 2019, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2019/329300c599/1gyrwFOAOfS)[ :octocat: ](https://github.com/irfanICMLL/structure_knowledge_distillation)
- **Knowledge Adaptation for Efficient Semantic Segmentation**, CVPR 2019, [ :link: ](https://www.semanticscholar.org/paper/Knowledge-Adaptation-for-Efficient-Semantic-He-Shen/a7e2c8c52c5a0202c11d9d6b3fe3427976608c1b)
- **Intra-class Feature Variation Distillation for Semantic Segmentation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21)[ :octocat: ](https://github.com/YukangWang/IFVD)
- **Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation**, WACVW 2020, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacvw/2021/196700a134/1sZ3qMT1big)[ :octocat: ](https://github.com/divyakraman/Domain-Adaptive-Knowledge-Distillation-for-Driving-Scene-Semantic-Segmentation)
- **Channel-wise Knowledge Distillation for Dense Prediction**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200f291/1BmEV5pb5S0)[ :octocat: ](https://github.com/drilistbox/CWD)
- **Double Similarity Distillation for Semantic Image Segmentation**, IEEE Transactions on Image Processing (TIP) 2021, [ :link: ](https://ieeexplore.ieee.org/iel7/83/9263394/09444191.pdf)
- **Robust Semantic Segmentation With Multi-Teacher Knowledge Distillation**, IEEE Access 2021, [ :link: ](https://ieeexplore.ieee.org/iel7/6287639/9312710/09522137.pdf)
- **Cross-Image Relational Knowledge Distillation for Semantic Segmentation**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600m2309/1H1lrFlWdSo)[ :octocat: ](https://github.com/winycg/CIRKD)
- **Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600q6855/1H0OftFDZiU)
- **Adaptive Perspective Distillation for Semantic Segmentation**, TPAMI 2022, [ :link: ](https://ieeexplore.ieee.org/document/9736597)[ :octocat: ](https://github.com/dvlab-research/APD)
- **Masked Generative Distillation**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_4)[ :octocat: ](https://github.com/yzd-v/MGD)
- **Class Similarity Weighted Knowledge Distillation for Continual Semantic Segmentation**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600q6845/1H1naA5zDji)
- **Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600j903/1H1iHG6Azfi)
- **DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900p5866/1POSJxCNhQI)[ :octocat: ](https://github.com/fy-vision/DiGA/blob/main/domain_generalization/README.md)
- **A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800l1686/1TJdwP4ftni)
- **Endpoints Weight Fusion for Class Incremental Semantic Segmentation**, CVPR 2023, [ :link: ](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf)[ :octocat: ](https://github.com/schuy1er/EWF_official)
- **Multi-Task Learning with Knowledge Distillation for Dense Prediction**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800v1493/1TJkQVn5EOs)
- **Hierarchical Dense Correlation Distillation for Few-Shot Segmentation**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900x3641/1POP1bpBIas)
- **Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900t9627/1POOu7ftY9W)[ :octocat: ](https://github.com/dahyun-kang/cst)
- **Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900h214/1PONQmM6id2)
- **FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200a584/1W0dpRl3Exa)
- **Knowledge Distillation for Efficient Instance Semantic Segmentation with Transformers**, CVPRW 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvprw/2024/654700f432/20A3pIxbIm4)
- **CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**, ICLR 2024, [ :link: ](https://wusize.github.io/publication/clipself)[ :octocat: ](https://github.com/wusize/CLIPSelf)
- **P2Seg: Pointly-supervised Segmentation via Mutual Distillation**, ICLR 2024, [ :link: ](https://arxiv.org/abs/2401.09709)[ :octocat: ](https://github.com/ucas-vg/P2Seg-Public)
- **Rethinking Knowledge Distillation With Raw Features for Semantic Segmentation**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200b144/1W0fgmH3wFW)
- **BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200b051/1W0cPBOBJlu)[ :octocat: ](https://github.com/AkideLiu/BPKD)
- **Guided Distillation for Semi-Supervised Instance Segmentation**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200a464/1W0eryUjSnK)[ :octocat: ](https://github.com/facebookresearch/GuidedDistillation)
- **Distilling efficient Vision Transformers from CNNs for semantic segmentation**, Pattern Recognition 2025, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0031320324007805)
- **AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation**, Multimedia Tools and Applications 2025, [ :link: ](https://link.springer.com/article/10.1007/s11042-025-20651-2)[ :octocat: ](https://github.com/AmirMansurian/AICSD)

### Depth Estimation

- **Knowledge Distillation for Fast and Accurate Monocular Depth Estimation on Mobile Devices**, CVPRW 2021, [ :link: ](https://ieeexplore.ieee.org/document/9523099/)
- **Realtime single image depth perception in the wild with handheld devices**, Sensors 2021, [ :link: ](https://www.semanticscholar.org/paper/71b44a90eca71b1d3cd65191ae0a18cfe3345815)
- **Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200p5540/1BmG7MXqUuY)[ :octocat: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200p5540/1BmG7MXqUuY)
- **Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-19769-7_41)
- **Attention-based depth distillation with 3d-aware positional encoding for monocular 3d object detection**, AAAI 2022, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/25391/25163)
- **Boosting Light-Weight Depth Estimation Via Knowledge Distillation**, KSEM 2023, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-40283-8_3)[ :octocat: ](https://github.com/JunjH/Boosting-Light-Weight-Depth-Estimation)
- **Urcdc-depth: Uncertainty rectified cross-distillation with cutflip for monocular depth estimation**, IEEE Transactions on Multimedia 2023, [ :link: ](https://ieeexplore.ieee.org/document/10234651/)[ :octocat: ](https://github.com/ShuweiShao/URCDC-Depth)
- **Multi-task learning with knowledge distillation for dense prediction**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800v1493/1TJkQVn5EOs)
- **Attention-Based Knowledge Distillation in Scene Recognition: The Impact of a DCT-Driven Loss**, IEEE Transactions on Circuits and Systems for Video Technology 2023, [ :link: ](https://ieeexplore.ieee.org/document/10054444/)
- **Monocular Depth Estimation from a Fisheye Camera Based on Knowledge Distillation**, Sensors 2023, [ :link: ](https://www.mdpi.com/1424-8220/23/24/9866)
- **3D Distillation: Improving Self-Supervised Monocular Depth Estimation on Reflective Surfaces**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800j099/1TJc7T3wveg)
- **Multi-Task Learning with Knowledge Distillation for Dense Prediction**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800v1493/1TJkQVn5EOs)
- **GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800q6163/1TJkJSjW7NC)[ :octocat: ](https://github.com/zxcqlf/GasMono)
- **Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation**, NIPS 2024, [ :link: ](https://albert100121.github.io/Depth-Anywhere/)[ :octocat: ](https://github.com/albert100121/Depth-Anywhere)
- **Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation**, ACCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-981-96-0969-7_8)
- **MonoProb: Self-Supervised Monocular Depth Estimation With Interpretable Uncertainty**, WACV 2024, [ :link: ](https://www.semanticscholar.org/paper/7b9cec6d1860ac0bbf9c52cec046c8b34a07a1ec)[ :octocat: ](https://github.com/CEA-LIST/MonoProb)
- **Monocular Depth Estimation via Self-Supervised Self-Distillation**, Sensors 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S1077314224001292)
- **MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation**, ICRA 2024, [ :link: ](https://www.semanticscholar.org/paper/MAL%3A-Motion-Aware-Loss-with-Temporal-and-Hints-for-Dong-Zhang/dc8f7eab03e39c7204820ea0790d9f0fcc863ce5)[ :octocat: ](https://github.com/YuejiangDong/MAL)
- **Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency**, ICASSP 2024, [ :link: ](https://www.semanticscholar.org/paper/b2824c0c78de7bd0dfc57c0518105d8ab8ee811d)

### Medical Image Analysis

- **Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification**, MICCAI 2021, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_16)[ :octocat: ](https://github.com/hathawayxxh/CRCKD)
- **Efficient Medical Image Segmentation Based on Knowledge Distillation**,  IEEE Transactions on Medical Imaging (TMI) 2021, [ :link: ](https://ieeexplore.ieee.org/iel7/42/9629464/09491090.pdf)[ :octocat: ](https://github.com/EagleMIT/EMKD)
- **DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation**, MICCAI 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-16440-8_52)[ :octocat: ](https://github.com/yeerwen/DeSD)
- **Efficient Biomedical Instance Segmentation via Knowledge Distillation**, MICCAI 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-16440-8_2)
- **Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification**, MICCAI 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-16437-8_21)
- **Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions**, MICCAI 2022, [ :link: ](https://xmengli.github.io/papers/2022MICCAI-XDing.pdf)[ :octocat: ](https://github.com/xmed-lab/DistillingSelf)
- **Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation**, IEEE Trans. Med. Imaging 2022, [ :link: ](https://www.semanticscholar.org/paper/SimCVD%3A-Simple-Contrastive-Voxel-Wise-Distillation-You-Zhou/6ea88feb2e6d47dfef9f00bfbff4f11c82d38fc1)
- **Deep Mutual Distillation for Semi-Supervised Medical Image Segmentation**, MICCAI 2023, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-43898-1_52)[ :octocat: ](https://github.com/SilenceMonk/Dual-Mutual-Distillation)
- **Distilling BlackBox to Interpretable models for Efficient Transfer Learning**, MICCAI 2023, [ :link: ](https://shantanu-ai.github.io/projects/MICCAI-2023-MoIE-CXR/static/data/Route-Interpret-Repeat-transfer-learning-miccai-23-poster-v1.pdf)[ :octocat: ](https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs)
- **Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality**, MICCAI 2023, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-43901-8_21)
- **Self-distillation for surgical action recognition**, MICCAI 2023, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-43996-4_61)[ :octocat: ](https://github.com/IMSY-DKFZ/self-distilled-swin)
- **Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions**, MICCAI 2023, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-43987-2_55)[ :octocat: ](https://github.com/HiLab-git/CDMA)
- **Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models**, CVPR 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000l238/20hNgAV7eTu)
- **Reverse Knowledge Distillation: Training a Large Model Using a Small One for Retinal Image Matching on Limited Data**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200h763/1W0cXTq40j6)
- **Learning Robust Shape Regularization for Generalizable Medical Image Segmentation**, IEEE Trans. Med. Imaging 2024, [ :link: ](https://ieeexplore.ieee.org/document/10457051)[ :octocat: ](https://github.com/tonyckc/WT-PSE-code/)
- **Enhancing Medical Imaging with GANs Synthesizing Realistic Images from Limited Data**, ICETCI 2024, [ :link: ](https://ieeexplore.ieee.org/iel8/10593799/10594004/10594540.pdf)
- **Exploring Generalizable Distillation for Efficient Medical Image Segmentation**, IEEE Journal of Biomedical and Health Informatics 2024, [ :link: ](https://ieeexplore.ieee.org/document/10491241)[ :octocat: ](https://github.com/XingqunQi-lab/GKD-Framework)
- **Confidence Matters: Enhancing Medical Image Classification Through Uncertainty-Driven Contrastive Self-Distillation**, MICCAI 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-72117-5_13)[ :octocat: ](https://github.com/philsaurabh/UDCD_MICCAI)
- **DES-SAM: Distillation-Enhanced Semantic SAM for Cervical Nuclear Segmentation with Box Annotation**, MICCAI 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-72114-4_22)[ :octocat: ](https://github.com/CVIU-CSU/DES-SAM)
- **Hallucinated Style Distillation for Single Domain Generalization in Medical Image Segmentation**, MICCAI 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-72117-5_41)
- **Progressively Correcting Soft Labels via Teacher Team for Knowledge Distillation in Medical Image Segmentation**, MICCAI 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-72114-4_50)
- **Reprogramming Distillation for Medical Foundation Models**, MICCAI 2024, [ :link: ](https://www.semanticscholar.org/paper/49d29ad12d3d4acf18334d824fe087f163a82eef)
- **HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical Image Classification**, Engineering Applications of Artificial Intelligence 2024, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0952197624015884)[ :octocat: ](https://github.com/omarsherif200/HDKD)
- **Shape-Intensity Knowledge Distillation For Robust Medical Image Segmentation**, Frontiers of Computer Science 2024, [ :link: ](https://link.springer.com/article/10.1007/s11704-024-40462-2)[ :octocat: ](https://github.com/whdong-whu/SIKD)

### Object Tracking

- **There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900l1607/1yeL4UQ401y)[ :octocat: ](https://github.com/robot-learning-freiburg/MM-DistillNet)
- **Ensemble learning with siamese networks for visual tracking**, Neurocomputing 2021, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0925231221012091)
- **Distilled Siamese Networks for Visual Tracking**, TPAMI 2022, [ :link: ](https://www.computer.org/csdl/journal/tp/2022/12/09612065/1yrD4rnPnji)
- **SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking**, CVPR 2024, [ :link: ](https://www.semanticscholar.org/paper/SDSTrack%3A-Self-Distillation-Symmetric-Adapter-for-Hou-Xing/3e8e7ef9ae6a1de0619b7c4cf5e1e8fc12252452)[ :octocat: ](https://github.com/hoqolo/SDSTrack)
- **Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline**, CVPR 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000t248/20hMuK7pjlS)[ :octocat: ](https://github.com/Event-AHU/EventVOT_Benchmark)
- **Object Knowledge Distillation for Joint Detection and Tracking in Satellite Videos**, IEEE Transactions on Geoscience and Remote Sensing  2024, [ :link: ](https://ieeexplore.ieee.org/iel7/36/10354519/10409214.pdf)

### Face Recognition

- **Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection**, ICCV 2019, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2019/480300a783/1hVluoqyK76)[ :octocat: ](https://github.com/D-X-Y/landmark-detection/blob/master/README.md)
- **Fair Feature Distillation for Visual Recognition**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900m2110/1yeLDUXHocw)
- **Rectifying the Data Bias in Knowledge Distillation**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccvw/2021/019100b477/1yNizRLoAcU)
- **Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-19775-8_37)[ :octocat: ](https://github.com/gist-ailab/teaching-where-to-look)
- **Evaluation-oriented knowledge distillation for deep face recognition**, CVPR 2022, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600s8719/1H1i2PcK9e8)
- **Rethinking Feature-Based Knowledge Distillation for Face Recognition**, CVPR 2023, [ :link: ](https://ieeexplore.ieee.org/document/10203514/)
- **Probabilistic Knowledge Distillation of Face Ensembles**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900d489/1POSs2wz09G)
- **SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data**,  IJCB 2023, [ :link: ](https://ieeexplore.ieee.org/iel7/10447019/10448524/10448642.pdf)[ :octocat: ](https://github.com/otroshi/synthdistill)
- **AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition**, ECCV 2024, [ :link: ](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07206.pdf)[ :octocat: ](https://github.com/fdbtrs/AdaDistill)
- **How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition**, ECCV Workshop 2024, [ :link: ](https://www.semanticscholar.org/paper/How-Knowledge-Distillation-Mitigates-the-Synthetic-Neto-Colakovic/b55f6a85e701b5f6888f17c1e60212caba480d69)[ :octocat: ](https://github.com/ivonacolakovic/SynthGap-mitigation-using-KD-in-FFR)
- **MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition**, ECCV Workshop 2024, [ :link: ](https://arxiv.org/abs/2408.16563)[ :octocat: ](https://github.com/eduardacaldeira/mst-kd)
- **Enhanced Face Recognition using Intra-class Incoherence Constraint**, ICLR 2024, [ :link: ](https://www.semanticscholar.org/paper/Enhanced-Face-Recognition-using-Intra-class-Huang-Wang/e77ea8899e20255a049aa9ee4f473c7415a06bf8)
- **ProS: Facial Omni-Representation Learning via Prototype-Based Self-Distillation**, WACV 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2024/189200g075/1W0cHrWzX4Q)
- **AI-KD: Towards Alignment Invariant Face Image Quality Assessment Using Knowledge Distillation**, IWBF 2024, [ :link: ](https://ieeexplore.ieee.org/iel8/10592864/10592870/10593907.pdf)[ :octocat: ](https://github.com/LSIbabnikz/AI-KD)

### Action Recognition

- **Structural Knowledge Distillation for Efficient Skeleton-Based Action Recognition**, IEEE Trans. Image Processing 2021, [ :link: ](https://ieeexplore.ieee.org/document/9351789/)[ :octocat: ](https://github.com/xiaochehe/SKD)
- **Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200j234/1BmI6MaLeO4)[ :octocat: ](https://github.com/jhong93/vpd)
- **Learning an Augmented RGB Representation with Cross-modal Knowledge Distillation for Action Detection**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200n3033/1BmLjRNSMCc)
- **Privileged Knowledge Distillation for Online Action Detection**, Pattern recognition 2022, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0031320322002229)
- **Multimodal Distillation for Egocentric Action Recognition**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800f190/1TJfgPEizmg)[ :octocat: ](https://github.com/gorjanradevski/multimodal-distillation)
- **FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2023/071800k0366/1TJgWLrpWmY)
- **Decomposed Cross-Modal Distillation for RGB-Based Temporal Action Detection**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900c373/1POR41xvAcg)
- **Generative Model-Based Feature Knowledge Distillation for Action Recognition**, AAAI 2024, [ :link: ](https://www.semanticscholar.org/paper/Generative-Model-based-Feature-Knowledge-for-Action-Wang-Zhao/b4986187b799aff146c5a7e7dbbb6cab9e0fb24a)[ :octocat: ](https://github.com/aaai-24/Generative-based-KD)
- **FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition**, ICLR 2024, [ :link: ](https://visual-ai.github.io/froster/)[ :octocat: ](https://github.com/Visual-AI/FROSTER)

### Pose Estimation

- **Dynamic Kernel Distillation for Efficient Pose Estimation in Videos**, ICCV 2019, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2019/480300g941/1hVls5NH4Xe)
- **Fast Human Pose Estimation**, CVPR 2019, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf)[ :octocat: ](https://github.com/ilovepose/fast-human-pose-estimation.pytorch)
- **Online Knowledge Distillation for Efficient Pose Estimation**, ICCV 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccv/2021/281200l1720/1BmFmqUmBEI)[ :octocat: ](https://github.com/zhengli97/OKDHP)
- **When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900l1850/1yeK5rpcWJy)[ :octocat: ](https://github.com/AIprogrammer/AdvMix)
- **From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation**, CVPR 2021, [ :link: ](https://chaneyddtt.github.io/UDA-animal-pose.github.io/)[ :octocat: ](https://github.com/chaneyddtt/UDA-Animal-Pose)
- **Combining Weight Pruning and Knowledge Distillation for CNN Compression**, CVPR Workshop 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvprw/2021/489900d185/1yVA3jwO6Wc)
- **AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time**, IEEE TPAMI 2022, [ :link: ](https://dl.acm.org/doi/10.1109/TPAMI.2022.3222784)[ :octocat: ](https://github.com/MVIG-SJTU/AlphaPose)
- **ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation**, NeurIPS 2022, [ :link: ](https://www.semanticscholar.org/paper/ViTPose%3A-Simple-Vision-Transformer-Baselines-for-Xu-Zhang/fdcad86866ca22d8417599deb41b54fe01487ce8)[ :octocat: ](https://github.com/ViTAE-Transformer/ViTPose)
- **DistilPose: Tokenized Pose Regression With Heatmap Distillation**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900c163/1POOJWOnLfW)[ :octocat: ](https://github.com/yshMars/DistilPose)
- **Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900s8633/1POOzo0l4Aw)[ :octocat: ](https://github.com/GUOShuxuan/kd-6d-pose-adlp)
- **Effective Whole-Body Pose Estimation with Two-Stages Distillation**, ICCV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/iccvw/2023/7.44E216/1TanA18CZCE)[ :octocat: ](https://github.com/IDEA-Research/DWPose)
- **HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation**, Chinese Journal of Electronics 2023, [ :link: ](https://ieeexplore.ieee.org/iel7/9970761/10038784/10038797.pdf)
- **SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation**, CVPR 2024, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000b082/20hRz5SEpfW)[ :octocat: ](https://github.com/MartyrPenink/SDPose)

### Image Retrieval

- **Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification**, AAAI 2020, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/6774)
- **Robust re-identification by multiple views knowledge distillation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58607-2_6)[ :octocat: ](https://github.com/aimagelab/VKD)
- **Relationship-Preserving Knowledge Distillation for Zero-Shot Sketch Based Image Retrieval**, ACMM 2021, [ :link: ](https://dl.acm.org/doi/10.1145/3474085.3475676)
- **Asymmetric metric learning for knowledge transfer**, CVPR 2021, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900i224/1yeKdzMHuBa)
- **Contextual Similarity Distillation for Asymmetric Image Retrieval**, CVPR 2022, [ :link: ](https://www.semanticscholar.org/paper/Contextual-Similarity-Distillation-for-Asymmetric-Wu-Wang/f65e4ec26c5be3671a8b39c236b7ecbd9a224f9a)
- **Large-to-small image resolution asymmetry in deep metric learning**, WACV 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/wacv/2023/934600b451/1L6LFCeS2Z2)
- **Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval**, CVPR 2023, [ :link: ](https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900q6006/1POV38eawUw)
- **D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval**, CVPR 2024, [ :link: ](https://ieeexplore.ieee.org/document/10656001/)
- **LSTKC: Long Short-Term Knowledge Consolidation for Lifelong Person Re-identification**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/29554)[ :octocat: ](https://ojs.aaai.org/index.php/AAAI/article/view/29554)
- **Pairwise difference relational distillation for object re-identification**, Pattern Recognition 2024, [ :link: ](https://www.sciencedirect.com/science/article/pii/S0031320324002061)
