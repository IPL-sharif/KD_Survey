# Algorithms

*  [Attention Distillation](#Attention-Distillation)
*  [Adversarial Distillation](#Adversarial-Distillation)
*  [Multi-teacher Distillation](#Multi-teacher-Distillation)
*  [Cross-modal Distillation](#Cross-modal-Distillation)
*  [Graph-based Distillation](#Graph-based-Distillation)
*  [Adaptive Distillation](#Adaptive-Distillation)
*  [Contrastive Distillation](#Contrastive-Distillation)
---
## Attention Distillation
- **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer**, ICLR 2017, [ :link: ](https://arxiv.org/abs/1612.03928)[ :octocat: ](https://github.com/szagoruyko/attention-transfer)
- **Knowledge Transfer with Jacobian Matching**, ICML 2018, [ :link: ](https://arxiv.org/abs/1803.00443)
- **Class Attention Map Distillation for Efficient Semantic Segmentation**, MVIP 2020, [ :link: ](https://ieeexplore.ieee.org/document/9116875)
- **Double Similarity Distillation for Semantic Image Segmentation**, TIP 2021, [ :link: ](https://arxiv.org/abs/2107.08591)
- **Efficient Semantic Segmentation via Self-Attention and Self-Distillation**, T-ITS 2022, [ :link: ](https://ieeexplore.ieee.org/document/9678134)
- **Class Attention Transfer for Semantic Segmentation**, AICAS 2022, [ :link: ](https://ieeexplore.ieee.org/document/9869901)[ :octocat: ](https://github.com/yubin1219/Semantic-Seg-KD)
- **Hierarchical Multi-Attention Transfer for Knowledge Distillation**, ACM MM 2023, [ :link: ](https://dl.acm.org/doi/10.1145/3568679)
- **Class Attention Transfer Based Knowledge Distillation**, CVPR 2023, [ :link: ](https://arxiv.org/abs/2304.12777)[ :octocat: ](https://github.com/GzyAftermath/CAT-KD)
- **Attention and feature transfer based knowledge distillation**, Nature 2023, [ :link: ](https://www.nature.com/articles/s41598-023-43986-y)
- **Student-friendly knowledge distillation**, Knowledge-Based Systems 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0950705124005495)
- **Channel-spatial knowledge distillation for efficient semantic segmentation**, Pattern Recognition 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S016786552400059X)[ :octocat: ](https://github.com/ayoubkarine/CSKD)
- **SAKD: Sparse attention knowledge distillation**, Image and Video Computing 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0262885624001240)
- **Knowledge Diffusion for Distillation**, NeurIPS 2024, [ :link: ](https://arxiv.org/abs/2305.15712)[ :octocat: ](https://github.com/hunto/DiffKD)
- **LAKD-Activation Mapping Distillation Based on Local Learning**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2408.11478)
- **Attention-guided Feature Distillation for Semantic Segmentation**, CVIU 2025, [ :link: ](https://arxiv.org/abs/2403.05451)[ :octocat: ](https://github.com/AmirMansurian/AttnFD)

## Adversarial Distillation
### Adversarial Data-free Distillation
- **Teacher-student compression with generative adversarial networks**, arXiv 2018, [ :link: ](https://arxiv.org/abs/1812.02271)[ :octocat: ](https://github.com/RuishanLiu/GAN-TSC)
- **Data-Free Adversarial Distillation**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1912.11006)[ :octocat: ](https://github.com/VainF/Data-Free-Adversarial-Distillation?tab=readme-ov-file)
- **Lifelong GAN: Continual Learning for Conditional Image Generation**, ICCV 2019, [ :link: ](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html)
- **Data-free learning of student networks**, ICCV 2019, [ :link: ](https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Data-Free_Learning_of_Student_Networks_ICCV_2019_paper.html)[ :octocat: ](https://github.com/huawei-noah/Efficient-Computing/tree/master/Data-Efficient-Model-Compression/DAFL)
- **Zero-shot knowledge transfer via adversarial belief matching**, NeurIPS 2019, [ :link: ](https://proceedings.neurips.cc/paper/2019/hash/fe663a72b27bdc613873fbbb512f6f67-Abstract.html)[ :octocat: ](https://github.com/polo5/ZeroShotKnowledgeTransfer)
- **Data-free knowledge amalgamation via group-stack dual-gan**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Ye_Data-Free_Knowledge_Amalgamation_via_Group-Stack_Dual-GAN_CVPR_2020_paper.html)
- **Data-Free Network Quantization With Adversarial Knowledge Distillation**, CVPR Workshop 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Choi_Data-Free_Network_Quantization_With_Adversarial_Knowledge_Distillation_CVPRW_2020_paper.html)
- **Dual Discriminator Adversarial Distillation for Data-free Model Compression**, IJMLC 2022, [ :link: ](https://link.springer.com/article/10.1007/s13042-021-01443-0)
- **Momentum adversarial distillation: Handling large distribution shifts in data-free knowledge distillation**, NeurIPS 2022, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2022/hash/41128e5b3a7622da5b17588757599077-Abstract-Conference.html)
- **Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers**, AAAI 2022, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/21423)
- **Learning To Retain While Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation**, CVPR 2023, [ :link: ](https://openaccess.thecvf.com/content/CVPR2023/html/Patel_Learning_To_Retain_While_Acquiring_Combating_Distribution-Shift_in_Adversarial_Data-Free_CVPR_2023_paper.html)
- **Out of thin air: Exploring data-free adversarial robustness distillation**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/28390)
- **Impartial Adversarial Distillation: Addressing Biased Data-Free Knowledge Distillation via Adaptive Constrained Optimization**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/28120)
- **DERD: Data-free Adversarial Robustness Distillation through Self-adversarial Teacher Group**, ACM Multimedia 2024, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3664647.3680796)

### Discriminator in the Loop
- **Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks**, arXiv 2017, [ :link: ](https://arxiv.org/abs/1709.00513)
- **Adversarial learning of portable student networks**, AAAI 2018, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/11667)
- **Adversarial distillation for efficient recommendation with external knowledge**, ACM TOIS 2018, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3281659)
- **Training student networks for acceleration with conditional adversarial networks**, BMVC 2018, [ :link: ](https://bmva-archive.org.uk/bmvc/2018/contents/papers/0154.pdf)
- **Adversarial network compression**, ECCV 2018, [ :link: ](https://openaccess.thecvf.com/content_eccv_2018_workshops/w23/html/Belagiannis_Adversarial_Network_Compression_ECCVW_2018_paper.html)
- **Kdgan: Knowledge distillation with generative adversarial networks**, NeurIPS 2018, [ :link: ](https://proceedings.neurips.cc/paper/2018/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html)
- **Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection**, AAAI 2019, [ :link: ](https://aaai.org/ojs/index.php/AAAI/article/view/4649)
- **Meal: Multi-model ensemble via adversarial learning**, AAAI 2019, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/4417)[ :octocat: ](https://github.com/AaronHeee/MEAL)
- **Knowledge squeezed adversarial network compression**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1904.05100)
- **structured knowledge distillation for semantic segmentation**, CVPR 2019, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html)[ :octocat: ](https://github.com/irfanICMLL/structure_knowledge_distillation)
- **Adversarial Distillation for Learning with Privileged Provisions**, IEEE TPAMI 2019, [ :link: ](https://ieeexplore.ieee.org/abstract/document/8845633)
- **Hierarchical Knowledge Squeezed Adversarial Network Compression**, AAAI 2020, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/6799)
- **Be Your Own Best Competitor! Multi-Branched Adversarial Knowledge Transfer**, arXiv 2020, [ :link: ](https://arxiv.org/abs/2010.04516)
- **AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distillation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58610-2_10)
- **Intra-class Feature Variation Distillation for Semantic Segmentation**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21)[ :octocat: ](https://github.com/YukangWang/IFVD)
- **Feature-map-level online adversarial knowledge distillation**, ICML 2020, [ :link: ](https://proceedings.mlr.press/v119/chung20a.html)
- **Gan-knowledge distillation for one-stage object detection**, IEEE Access 2020, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9046859)
- **KTAN: Knowledge Transfer Adversarial Network**, IJCNN 2020, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9207235)
- **Lightning fast video anomaly detection via multi-scale adversarial distillation**, Elsevier CVIU 2024, [ :link: ](https://www.sciencedirect.com/science/article/pii/S1077314224001553)[ :octocat: ](https://github.com/ristea/fast-aed)

### Generative Adversarial Network Distillation
- **Compressing gans using knowledge distillation**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1902.00159)
- **MineGAN: effective knowledge transfer from GANs to target domains with few images**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_MineGAN_Effective_Knowledge_Transfer_From_GANs_to_Target_Domains_With_CVPR_2020_paper.html)[ :octocat: ](https://github.com/yaxingwang/MineGAN)
- **Gan compression: Efficient architectures for interactive conditional gans**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_GAN_Compression_Efficient_Architectures_for_Interactive_Conditional_GANs_CVPR_2020_paper.html)[ :octocat: ](https://github.com/mit-han-lab/gan-compression)
- **Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation**, CVPR 2022, [ :link: ](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Wavelet_Knowledge_Distillation_Towards_Efficient_Image-to-Image_Translation_CVPR_2022_paper.html)[ :octocat: ](https://github.com/ArchipLab-LinfengZhang/wkd-datasets)

### Diffusion Distillation
- **Sdxl-lightning: Progressive adversarial diffusion distillation**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2402.13929)[ :octocat: ](https://github.com/inferless/SDXL-Lightning)
- **CAD: Photorealistic 3D Generation via Adversarial Distillation**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Wan_CAD_Photorealistic_3D_Generation_via_Adversarial_Distillation_CVPR_2024_paper.html)[ :octocat: ](https://github.com/raywzy/CAD)
- **Adversarial Score Distillation: When score distillation meets GAN**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Wei_Adversarial_Score_Distillation_When_score_distillation_meets_GAN_CVPR_2024_paper.html)[ :octocat: ](https://github.com/2y7c3/ASD)
- **ACT-Diffusion: Efficient Adversarial Consistency Training for One-step Diffusion Models**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Kong_ACT-Diffusion_Efficient_Adversarial_Consistency_Training_for_One-step_Diffusion_Models_CVPR_2024_paper.html)[ :octocat: ](https://github.com/kong13661/ACT)
- **Learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-72667-5_4)
- **Adversarial Diffusion Distillation**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-73016-0_6)

### Adversarial Robust Distillation
- **Adversarially Robust Distillation**, AAAI 2020, [ :link: ](https://aaai.org/ojs/index.php/AAAI/article/view/5816)[ :octocat: ](https://github.com/goldblum/AdversariallyRobustDistillation)
- **Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Zi_Revisiting_Adversarial_Robustness_Distillation_Robust_Soft_Labels_Make_Student_Better_ICCV_2021_paper.html)[ :octocat: ](https://github.com/zibojia/RSLAD)
- **AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-Directional Metric Learning**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_AGKD-BML_Defense_Against_Adversarial_Attack_by_Attention_Guided_Knowledge_Distillation_ICCV_2021_paper.html)[ :octocat: ](https://github.com/hongw579/AGKD-BML)
- **Enhanced Accuracy and Robustness via Multi-Teacher Adversarial Distillation**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-19772-7_34)[ :octocat: ](https://github.com/zhaoshiji123/MTARD)
- **Reliable Adversarial Distillation with Unreliable Teachers**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2106.04928)
- **Indirect Gradient Matching for Adversarial Robust Distillation**, arXiv 2023, [ :link: ](https://arxiv.org/abs/2312.03286)
- **Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation**, CVPR 2023, [ :link: ](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Boosting_Accuracy_and_Robustness_of_Student_Models_via_Adaptive_Adversarial_CVPR_2023_paper.html)[ :octocat: ](https://github.com/boyellow/AdaAD)
- **Adversarial Local Distribution Regularization for Knowledge Distillation**, WACV 2023, [ :link: ](https://openaccess.thecvf.com/content/WACV2023/html/Nguyen-Duc_Adversarial_Local_Distribution_Regularization_for_Knowledge_Distillation_WACV_2023_paper.html)
- **Adversarial Distillation Based on Slack Matching and Attribution Region Alignment**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Yin_Adversarial_Distillation_Based_on_Slack_Matching_and_Attribution_Region_Alignment_CVPR_2024_paper.html)[ :octocat: ](https://github.com/InsLin/SmaraAD)
- **Robust Distillation via Untargeted and Targeted Intermediate Adversarial Samples**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Dong_Robust_Distillation_via_Untargeted_and_Targeted_Intermediate_Adversarial_Samples_CVPR_2024_paper.html)
- **PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor**, CVPR 2024, [ :link: ](https://openaccess.thecvf.com/content/CVPR2024/html/Jung_PeerAiD_Improving_Adversarial_Distillation_from_a_Specialized_Peer_Tutor_CVPR_2024_paper.html)[ :octocat: ](https://github.com/jaewonalive/PeerAiD)
- **Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-73220-1_12)[ :octocat: ](https://github.com/kunsaram01/DGAD)
- **Adversarially Robust Distillation by Reducing the Student-Teacher Variance Gap**, ECCV 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-73235-5_6)

## Multi-teacher Distillation
- **Amalgamating knowledge towards comprehensive classification**, AAAI 2019, [ :link: ](https://dl.acm.org/doi/10.1609/aaai.v33i01.33013068)
- **Knowledge amalgamation from heterogeneous networks by common feature learning**, IJCAI 2019, [ :link: ](https://dl.acm.org/doi/10.5555/3367243.3367468)[ :octocat: ](https://github.com/VainF/CommonFeatureLearning)
- **Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1908.07121)[ :octocat: ](https://github.com/UpCoder/KnowledgeAmalgamationModule)
- **Unifying Heterogeneous Classifiers with Distillation**, CVPR 2019, [ :link: ](https://arxiv.org/abs/1904.06062)
- **A Two-Teacher Framework for Knowledge Distillation**, ISNN 2019, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-22796-8_7)
- **Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning**, ECCV 2020, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-58539-6_38) 
- **Ensemble Knowledge Distillation for Learning Improved and Efficient Networks**, ECAI 2020, [ :link: ](https://arxiv.org/abs/1909.08097) 
- **FEED: Feature-level Ensemble for Knowledge Distillation**, ECAI 2020, [ :link: ](https://arxiv.org/abs/1909.10754) 
- **Robust Semantic Segmentation With Multi-Teacher Knowledge Distillation**, IEEE Access 2021, [ :link: ](https://ieeexplore.ieee.org/document/9522137)
- **Knowledge distillation guided by multiple homogeneous teachers**, Information Science 2022, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0020025522005576)
- **MTED: multiple teachers ensemble distillation for compact semantic segmentation**, Neural Computing 2023, [ :link: ](https://link.springer.com/article/10.1007/s00521-023-08321-6) 
- **Multi-teacher knowledge distillation based on joint Guidance of Probe and Adaptive Corrector**, Neural Networks 2023, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0893608023002010)
- **Knowledge Amalgamation for Object Detection With Transformers**, TIP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10091778)[ :octocat: ](https://github.com/zhfeing/Transformer-KA-PyTorch)
- **Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation**, ICML 2023, [ :link: ](https://arxiv.org/abs/2308.09105)[ :octocat: ](https://github.com/Shengcao-Cao/MTPD)
- **Adaptive Multi-Teacher Knowledge Distillation with Meta-Learning**, ICME 2023, [ :link: ](https://ieeexplore.ieee.org/document/10219996)[ :octocat: ](https://github.com/Rorozhl/MMKD)
- **ATMKD: adaptive temperature guided multi-teacher knowledge distillation**, Multimedia Systems 2024, [ :link: ](https://link.springer.com/article/10.1007/s00530-024-01483-w) 
- **Amalgamating Knowledge for Comprehensive Classification with Uncertainty Suppression**, ISCAS 2024, [ :link: ](https://ieeexplore.ieee.org/document/10557913/) 
- **Relation-Based Multi-Teacher Knowledge Distillation**, IJCNN 2024, [ :link: ](https://ieeexplore.ieee.org/document/10650189) 
- **DE-MKD: Decoupled Multi-Teacher Knowledge Distillation Based on Entropy**, Mathematics 2024, [ :link: ](https://www.mdpi.com/2227-7390/12/11/1672) 

## Cross-modal Distillation

- **Cross Modal Distillation for Supervision Transfer**, CVPR 2016, [ :link: ](http://openaccess.thecvf.com/content_cvpr_2016/html/Gupta_Cross_Modal_Distillation_CVPR_2016_paper.html)[ :octocat: ](https://github.com/s-gupta/fast-rcnn)
- **Modality distillation with multiple stream networks for action recognition**, ECCV 2018, [ :link: ](http://openaccess.thecvf.com/content_ECCV_2018/html/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.html)[ :octocat: ](https://github.com/ncgarcia/modality-distillation)
- **Learning with Privileged Information via Adversarial Discriminative Modality Distillation**, TPAMI 2019, [ :link: ](https://ieeexplore.ieee.org/abstract/document/8764498/)[ :octocat: ](https://github.com/pmorerio/admd)
- **Cross-Modal Knowledge Distillation for Action Recognition**, ICIP 2019, [ :link: ](https://ieeexplore.ieee.org/abstract/document/8802909/)
- **3D-to-2D Distillation for Indoor Scene Parsing**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_3D-to-2D_Distillation_for_Indoor_Scene_Parsing_CVPR_2021_paper.html?ref=https://githubhelp.com)[ :octocat: ](https://github.com/liuzhengzhe/3D-to-2D-Distillation-for-Indoor-Scene-Parsing)
- **EvDistill: Asynchronous Events To End-Task Learning via Bidirectional Reconstruction-Guided Cross-Modal Knowledge Distillation**, CVPR 2021, [ :link: ](http://openaccess.thecvf.com/content/CVPR2021/html/Wang_EvDistill_Asynchronous_Events_To_End-Task_Learning_via_Bidirectional_Reconstruction-Guided_Cross-Modal_CVPR_2021_paper.html)[ :octocat: ](https://github.com/addisonwang2013/evdistill)
- **Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20080-9_6)[ :octocat: ](https://github.com/Cc-Hy/CMKD)
- **Robust Cross-Modal Representation Learning with Progressive Self-Distillation**, CVPR 2022, [ :link: ](http://openaccess.thecvf.com/content/CVPR2022/html/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.html) 
- **UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View**, CVPR 2023, [ :link: ](http://openaccess.thecvf.com/content/CVPR2023/html/Zhou_UniDistill_A_Universal_Cross-Modality_Knowledge_Distillation_Framework_for_3D_Object_CVPR_2023_paper.html)[ :octocat: ](https://github.com/megvii-research/cvpr2023-unidistill)
- **DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation**, ICCV 2023, [ :link: ](http://openaccess.thecvf.com/content/ICCV2023/html/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.html)[ :octocat: ](https://github.com/qcraftai/distill-bev)
- **X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection**, CVPR 2023, [ :link: ](http://openaccess.thecvf.com/content/CVPR2023/html/Klingner_X3KD_Knowledge_Distillation_Across_Modalities_Tasks_and_Stages_for_Multi-Camera_CVPR_2023_paper.html)
- **Efficient RGB-T Tracking via Cross-Modality Distillation**, CVPR 2023, [ :link: ](http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.html)
- **Decomposed Cross-Modal Distillation for RGB-Based Temporal Action Detection**, CVPR 2023, [ :link: ](http://openaccess.thecvf.com/content/CVPR2023/html/Lee_Decomposed_Cross-Modal_Distillation_for_RGB-Based_Temporal_Action_Detection_CVPR_2023_paper.html)
- **STXD: structural and temporal cross-modal distillation for multi-view 3D object detection**, NeurIPS 2023, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5d8c01de2dc698c54201c1c7d0b86974-Abstract-Conference.html) 
- **C2KD: Bridging the Modality Gap for Cross-Modal Knowledge Distillation**, CVPR 2024, [ :link: ](http://openaccess.thecvf.com/content/CVPR2024/html/Huo_C2KD_Bridging_the_Modality_Gap_for_Cross-Modal_Knowledge_Distillation_CVPR_2024_paper.html)
- **CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation**, CVPR 2024, [ :link: ](http://openaccess.thecvf.com/content/CVPR2024/html/Zhao_CRKD_Enhanced_Camera-Radar_Object_Detection_with_Cross-modality_Knowledge_Distillation_CVPR_2024_paper.html)[ :octocat: ](https://github.com/Song-Jingyu/CRKD)
- **Xkd: Cross-modal knowledge distillation with domain alignment for video representation learning**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/29407)[ :octocat: ](https://github.com/pritamqu/XKD)
- **Radocc: Learning cross-modality occupancy knowledge through rendering assisted distillation**, AAAI 2024, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/28533) 
- **CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation**, TMM 2024, [ :link: ](https://ieeexplore.ieee.org/abstract/document/10413654/)



## Graph-based Distillation
- **Graph Distillation for Action Detection with Privileged Modalities**, ECCV 2018, [ :link: ](https://openaccess.thecvf.com/content_ECCV_2018/html/Zelun_Luo_Graph_Distillation_for_ECCV_2018_paper.html)[ :octocat: ](https://github.com/google/graph_distillation)
- **Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification**, IJCAI 2018, [ :link: ](https://arxiv.org/abs/1804.10069)
- **Knowledge Transfer Graph for Deep Collaborative Learning**, ACCV 2020, [ :link: ](https://openaccess.thecvf.com/content/ACCV2020/html/Minami_Knowledge_Transfer_Graph_for_Deep_Collaborative_Learning_ACCV_2020_paper.html)[ :octocat: ](https://github.com/somaminami/DCL)
- **Learning Student Networks via Feature Embedding**, IEEE TNNLS 2020, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9007474)

### Relational Knowledge Distillation via Graph
- **A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning**, CVPR 2017, [ :link: ](https://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)
- **Relational Knowledge Distillation**, CVPR 2019, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html)[ :octocat: ](https://github.com/lenscloth/RKD)
- **Similarity-Preserving Knowledge Distillation**, ICCV 2019, [ :link: ](https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html)
- **Correlation Congruence for Knowledge Distillation**, ICCV 2019, [ :link: ](https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.html)
- **Knowledge Distillation via Instance Relationship Graph**, CVPR 2019, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.html)[ :octocat: ](https://github.com/yufanLIU/IRG)
- **Graph-based Knowledge Distillation by Multi-head Attention Network**, BMVC 2019, [ :link: ](https://arxiv.org/abs/1907.02226)
- **Binarized Collaborative Filtering with Distilling Graph Convolutional Networks**, IJCAI 2019, [ :link: ](https://arxiv.org/abs/1906.01829)
- **Spatio-Temporal Graph for Video Captioning With Knowledge Distillation**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.html)[ :octocat: ](https://github.com/StanfordVL/STGraph)
- **Heterogeneous Knowledge Distillation Using Information Flow Modeling**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.html)[ :octocat: ](https://github.com/passalis/pkth)
- **Probabilistic Knowledge Transfer for Lightweight Deep Representation Learning**, IEEE TNNLS 2020, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9104915)[ :octocat: ](https://github.com/passalis/probabilistic_kt)
- **Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction**, AAAI 2020, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/6509)
- **Distilling Holistic Knowledge With Graph Neural Networks**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_Distilling_Holistic_Knowledge_With_Graph_Neural_Networks_ICCV_2021_paper.html)[ :octocat: ](https://github.com/wyc-ruiker/HKD)
- **Deep Structured Instance Graph for Distilling Object Detectors**, ICCV 2021, [ :link: ](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Deep_Structured_Instance_Graph_for_Distilling_Object_Detectors_ICCV_2021_paper.html)[ :octocat: ](https://github.com/dvlab-research/Dsig)
- **Dark Reciprocal-Rank: Teacher-to-student Knowledge Transfer from Self-localization Model to Graph-convolutional Neural Network**, IEEE ICRA 2021, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9561158)[ :octocat: ](https://github.com/KojiTakeda00/Reciprocal_rank_KT_GCN)
- **GKD: Semi-supervised Graph Knowledge Distillation for Graph-Independent Inference**, MICCAI 2021, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_68)[ :octocat: ](https://github.com/mahsa91/GKD-MICCAI2021)
- **Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network**, AAAI 2021, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/17009)[ :octocat: ](https://github.com/sseung0703/IEPKT)

### Graph Neural Network Distillation
- **Distilling Knowledge From Graph Convolutional Networks**, CVPR 2020, [ :link: ](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Distilling_Knowledge_From_Graph_Convolutional_Networks_CVPR_2020_paper.html)[ :octocat: ](https://github.com/ihollywhy/DistillGCN.PyTorch)
- **Graph few-shot learning via knowledge transfer**, AAAI 2020, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/6142)[ :octocat: ](https://github.com/huaxiuyao/GFL)
- **Tinygnn: Learning efficient graph neural networks**, ACM SIGKDD 2020, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3394486.3403236)
- **Reliable data distillation on graph convolutional network**, ACM SIGMOD 2020, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3318464.3389706)
- **Extract the knowledge of graph neural networks and go beyond it: An effective knowledge distillation framework**, ACM WWW 2021, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3442381.3450068)[ :octocat: ](https://github.com/BUPT-GAMMA/CPF)
- **Graph-free knowledge distillation for graph neural networks**, IJCAI 2021, [ :link: ](https://arxiv.org/abs/2105.07519)[ :octocat: ](https://github.com/Xiang-Deng-DL/GFKD)
- **MulDE: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings**, ACM WWW 2021, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3442381.3449898)
- **On self-distilling graph neural network**, IJCAI 2021, [ :link: ](https://arxiv.org/abs/2011.02255)
- **Edge: Enriching knowledge graph embeddings with external text**, NAACL 2021, [ :link: ](https://arxiv.org/abs/2104.04909)
- **ROD: Reception-aware Online Distillation for Sparse Graphs**, ACM SIGKDD 2021, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3447548.3467221)[ :octocat: ](https://github.com/zwt233/ROD)
- **Graph-less neural networks: Teaching old mlps new tricks via distillation**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2110.08727)[ :octocat: ](https://github.com/snap-research/graphless-neural-networks)
- **Cold brew: Distilling graph node representations with incomplete or missing neighborhoods**, ICLR 2022, [ :link: ](https://arxiv.org/abs/2111.04840)[ :octocat: ](https://github.com/amazon-science/gnn-tail-generalization)
- **On representation knowledge distillation for graph neural networks**, IEEE TNNLS 2022, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9969463/)[ :octocat: ](https://github.com/chaitjo/efficient-gnns)
- **Knowledge distillation improves graph structure augmentation for graph neural networks**, NeurIPS 2022, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2022/hash/4d4a3b6a34332d80349137bcc98164a5-Abstract-Conference.html)[ :octocat: ](https://github.com/LirongWu/KDGA)
- **Lte4g: Long-tail experts for graph neural networks**, ACM CIKM 2022, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3511808.3557381)[ :octocat: ](https://github.com/SukwonYun/LTE4G)
- **Sail: Self-augmented graph contrastive learning**, AAAI 2022, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/20875)
- **Compressing Deep Graph Neural Networks via Adversarial Knowledge Distillation**, ACM SIGKDD 2022, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3534678.3539315)[ :octocat: ](https://github.com/MIRALab-USTC/GraphAKD)
- **Collaborative Knowledge Distillation for Heterogeneous Information Network Embedding**, ACM WWW 2022, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3485447.3512209)[ :octocat: ](https://github.com/zhoushengisnoob/CKD)
- **FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks**, ACM SIGKDD 2022, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3534678.3539320)
- **Geometric Knowledge Distillation: Topology Compression for Graph Neural Networks**, NeurIPS 2022, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c06f788963f0ce069f5b2dbf83fe7822-Abstract-Conference.html)[ :octocat: ](https://github.com/chr26195/GKD)
- **Data-Free Adversarial Knowledge Distillation for Graph Neural Networks**, IJCAI 2022, [ :link: ](https://arxiv.org/abs/2205.03811)
- **Multi-Scale Distillation from Multiple Graph Neural Networks**, AAAI 2022, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/20354)[ :octocat: ](https://github.com/NKU-IIPLab/MSKD)
- **Alignahead: Online Cross-Layer Knowledge Extraction on Graph Neural Networks**, IJCNN 2022, [ :link: ](https://ieeexplore.ieee.org/abstract/document/9892159)[ :octocat: ](https://github.com/GuoJY-eatsTG/Alignahead)
- **Iterative Graph Self-Distillation**, ICLR 2023, [ :link: ](https://ieeexplore.ieee.org/abstract/document/10216383)
- **T2-GNN: Graph Neural Networks for Graphs with Incomplete Features and Structure via Teacher-Student Distillation**, AAAI 2023, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/25553)
- **Boosting Graph Neural Networks via Adaptive Knowledge Distillation**, AAAI 2023, [ :link: ](https://ojs.aaai.org/index.php/AAAI/article/view/25944)[ :octocat: ](https://github.com/devmehta01/Boosting-Graph-Neural-Networks-via-Adaptive-Knowledge-Distillation)
- **RELIANT: Fair Knowledge Distillation for Graph Neural Networks**, SDM 2023, [ :link: ](https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch18)[ :octocat: ](https://github.com/yushundong/RELIANT)
- **NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs**, ICLR 2023, [ :link: ](https://arxiv.org/abs/2208.10010)[ :octocat: ](https://github.com/meettyj/NOSMOG)
- **The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation**, ACM WSDM 2024, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3616855.3635768)
- **Online adversarial knowledge distillation for graph neural networks**, Elsevier ESWA 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0957417423021735)[ :octocat: ](https://github.com/wangz3066/OnlineDistillGCN)

## Adaptive Distillation
- **On the Efficacy of Knowledge Distillation**, ICCV 2019, [ :link: ](https://arxiv.org/abs/1910.01348)
- **Knowledge Distillation for Semantic Segmentation Using Channel and Spatial Correlations and Adaptive Cross Entropy**, Sensors 2019, [ :link: ](https://www.mdpi.com/1424-8220/20/16/4616)
- **Channel-wise attention for knowledge distillation**, arXiv 2020, [ :link: ](https://arxiv.org/abs/2006.01683)
- **Improved Knowledge Distillation via Teacher Assistant**, NeurIPS 2020, [ :link: ](https://arxiv.org/abs/1902.03393)[ :octocat: ](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)
- **Heterogeneous Knowledge Distillation using Information Flow Modeling**, ECCV 2020, [ :link: ](https://arxiv.org/abs/2005.00727)[ :octocat: ](https://github.com/passalis/pkth)
- **InDistill: Information flow-preserving knowledge distillation for model compression**, arXiv 2022, [ :link: ](https://arxiv.org/abs/2205.10003)
- **Prune Your Model Before Distill It**, ECCV 2022, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_8)[ :octocat: ](https://github.com/ososos888/prune-then-distill)
- **Channel-Correlation-Based Selective Knowledge Distillation**, T-CDS 2022, [ :link: ](https://ieeexplore.ieee.org/document/10000398/)[ :octocat: ](https://github.com/gjplab/CCSKD)
- **Adaptive Perspective Distillation for Semantic Segmentation**, T-MAPI 2022, [ :link: ](https://ieeexplore.ieee.org/document/9736597)[ :octocat: ](https://github.com/dvlab-research/APD)
- **Category correlation and adaptive knowledge distillation for compact cloud detection in remote sensing images**, T-GRS 2022, [ :link: ](https://ieeexplore.ieee.org/document/9774409)
- **Masked Distillation with Receptive Tokens**, ICLR 2023, [ :link: ](https://arxiv.org/abs/2205.14589)[ :octocat: ](https://github.com/hunto/MasKD)
- **RdimKD: Generic Distillation Paradigm by Dimensionality Reduction**, arXiv 2023, [ :link: ](https://arxiv.org/abs/2312.08700)
- **Knowledge Distillation with Active Exploration and Self-Attention Based Inter-Class Variation Transfer for Image Segmentation**, ICASSP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10097262)
- **Holistic Weighted Distillation for Semantic Segmentation**, ICME 2023, [ :link: ](https://ieeexplore.ieee.org/document/10220035)[ :octocat: ](https://github.com/zju-SWJ/HWD)
- **Enlightening the Student in Knowledge Distillation**, ICASSP 2023, [ :link: ](https://ieeexplore.ieee.org/document/10095328)[ :octocat: ](https://github.com/YujieZheng99/KDrefine)
- **Multi-teacher knowledge distillation based on joint Guidance of Probe and Adaptive Corrector**, Neural Networks 2023, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0893608023002010)
- **Attention and feature transfer based knowledge distillation**, Nature 2023, [ :link: ](https://www.nature.com/articles/s41598-023-43986-y)
- **Similarity Knowledge Distillation with Calibrated Mask**, ICASSP 2024, [ :link: ](https://cmsworkshops.com/ICASSP2024/view_paper.php?PaperNum=3324)
- **Coordinate Attention Guided Dual-Teacher Adaptive Knowledge Distillation for image classification**, ESWA 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0957417424007589)[ :octocat: ](https://github.com/mdt1219/CAG-DAKD/)
- **Prune Channel And Distill: Discriminative Knowledge Distillation For Semantic Segmentation**, ICIP 2024, [ :link: ](https://cmsworkshops.com/ICIP2024/view_paper.php?PaperNum=1987)
- **CS-KD: Confused Sample Knowledge Distillation for Semantic Segmentation of Aerial Imagery**, ICIC 2024, [ :link: ](https://link.springer.com/chapter/10.1007/978-981-97-5600-1_23)
- **Adaptive class token knowledge distillation for efficient vision transformer**, Knowledge-Based Systems 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0950705124011651) 
- **AFMPM: adaptive feature map pruning method based on feature distillation**, IJMLC 2024, [ :link: ](https://link.springer.com/article/10.1007/s13042-023-01926-2)
- **Coordinate Attention Guided Dual-Teacher Adaptive Knowledge Distillation for image classification**, ESWA 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0957417424007589)
- **Maximizing discrimination capability of knowledge distillation with energy function**, Knowledge-Based Systems 2024, [ :link: ](https://www.sciencedirect.com/science/article/abs/pii/S0950705124005458)[ :octocat: ](https://github.com/Seonghak35/EnergyKD)
- **Adaptive Inter-Class Similarity Distillation for Semantic Segmentation**, MTAP 2025, [ :link: ](https://link.springer.com/article/10.1007/s11042-025-20651-2)[ :octocat: ](https://github.com/AmirMansurian/AICSD)

## Contrastive Distillation
- **Knowledge Distillation for Single Image Super-Resolution via Contrastive Learning**, ACM 2024, [ :link: ](https://dl.acm.org/doi/abs/10.1145/3652583.3657606)
- **ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model**, arXiv 2024, [ :link: ](https://arxiv.org/abs/2408.04145)
- **Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient Semantic Segmentation**, NeurIPS 2023, [ :link: ](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a12779b5e802668df1cbc73fa00da62f-Abstract-Conference.html) [ :octocat: ](https://github.com/OSVAI/Af-DCD)
- **Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Improving_Weakly_Supervised_Visual_Grounding_by_Contrastive_Knowledge_Distillation_CVPR_2021_paper.html) [ :octocat: ](https://github.com/jhuang81/weak-sup-visual-grounding)
- **Complementary Relation Contrastive Distillation**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Complementary_Relation_Contrastive_Distillation_CVPR_2021_paper.html) [ :octocat: ](https://github.com/Lechatelia/CRCD)
- **Wasserstein Contrastive Representation Distillation**, CVPR 2021, [ :link: ](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Wasserstein_Contrastive_Representation_Distillation_CVPR_2021_paper.html)
- **Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification**, MICCAI 2021, [ :link: ](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_16) [ :octocat: ](https://github.com/hathawayxxh/CRCKD)
- **DistilCSE: Effective Knowledge Distillation For Contrastive Sentence Embeddings**, arXiv 2021, [ :link: ](https://arxiv.org/abs/2112.05638)
- **Contrastive Representation Distillation**, arXiv 2019, [ :link: ](https://arxiv.org/abs/1910.10699) [ :octocat: ](https://github.com/HobbitLong/RepDistiller)
- **Dimensionality Reduction by Learning an Invariant Mapping**, CVPR 2006, [ :link: ](https://ieeexplore.ieee.org/abstract/document/1640964)
 
